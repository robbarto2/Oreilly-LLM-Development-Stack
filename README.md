# The LLM Development Stack

This is the official GitHub repository for the Pearson Live Training course “The LLM Development Stack,” created by Rob Barton and Jerome Henry.

This course introduces developers and engineers to the tools, platforms, and frameworks that make up the modern LLM development stack. You’ll explore hands-on environments like Google Colab and Hugging Face, local model serving platforms like Ollama and LM Studio, agentic AI frameworks like LangChain and CrewAI, and cutting-edge communication protocols like Message Chain Protocol (MCP) and Agent-to-Agent (A2A). You’ll also learn how to benchmark and evaluate LLMs, and how to build full-stack generative AI apps in the cloud using AWS Bedrock, Azure AI Studio, and Google Vertex.

The course is designed to be hands-on and practical, with live demos and labs throughout.

---

## Authors

- Rob Barton, Cisco Distinguished Engineer  
- Jerome Henry, Cisco Principal Engineer

---

## Prerequisites

To get the most from this course, you should have:

- Basic Python skills  
- Familiarity with AI/ML concepts  
- A GitHub account and access to Google Colab

---

## Repository Structure

This repository is organized by section. Each section contains example code, labs, and demonstrations that are aligned with the course modules.

Section-1-Foundational-Tools/
Section-2-LLM-Management-Platforms/
Section-3-Agentic-Development-Tools/
Section-4-Agentic-Protocols/
Section-5-Cloud-LLM-Platforms/
Section-6-LLM-Evaluation/
Slides/
README.md


---

## Getting Started

1. Clone the repository:

git clone https://github.com/robbarto2/Oreilly-LLM-Development-Stack.git
cd Oreilly-LLM-Development-Stack


2. Open the appropriate section folder.

3. Use Google Colab or VS Code to run the provided notebooks and code samples.

---

## Course Outline

### Section 1: LLM Development Foundational Tools
This section is a combination of demo and slides, introducing the environments and platforms used throughout the course.

- Google Colab  
- Hugging Face  
- Q&A

---

### Section 2: LLM Management Platforms
Learn how to run LLMs locally using modern tools for inference and UI.

- Overview of Ollama and LMStudio  
- Ollama deep dive  
- Developing GenAI apps using Ollama and Open WebUI  
- Q&A

---

### Section 3: Agentic AI Development Tools
Build intelligent AI agents using Python and agentic development frameworks.

- Agentic frameworks: LangChain/LangGraph, CrewAI, AutoGen  
- Automating agents with LangChain  
- Programming your own agent with Python  
- Q&A

---

### Section 4: Agentic AI Development Protocols
Explore how agents interact and share context using cutting-edge protocols.

- Agentic Protocols  
- Message Chain Protocol (MCP)  
- Agent to Agent (A2A)  
- Model Context Protocol (MCP)  
- Q&A

---

### Section 5: Cloud-based LLM Development Platforms
Deploy and scale your AI applications using cloud-based LLM tools.

- Overview of Cloud LLM development tools (Google Vertex, Azure AI Studio, AWS Bedrock)  
- AWS Bedrock deep dive  
- Building an AI Assistant to use Bedrock  
- Q&A

---

### Section 6: LLM Evaluation and Benchmarking
Learn how to evaluate and benchmark your LLM-based applications.

- LLM Benchmarking Overview  
- Methods to benchmark performance of LLMs  
- Q&A

---

> This repository will be updated regularly as the course progresses.
