{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc7396e6-923e-4e9c-8dd6-73c433657e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wf/jmbr4tf11xj6_b2633x2y_8m0000gn/T/ipykernel_20849/597022834.py:15: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Could not import chromadb python package. Please install it with `pip install chromadb`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/langchain_community/vectorstores/chroma.py:83\u001b[0m, in \u001b[0;36mChroma.__init__\u001b[0;34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client, relevance_score_fn)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mchromadb\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'chromadb'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m docs \u001b[38;5;241m=\u001b[39m splitter\u001b[38;5;241m.\u001b[39msplit_documents(raw_docs)\n\u001b[1;32m     15\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m OllamaEmbeddings(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmxbai-embed-large\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m vs \u001b[38;5;241m=\u001b[39m \u001b[43mChroma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlc_chroma\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m retriever \u001b[38;5;241m=\u001b[39m vs\u001b[38;5;241m.\u001b[39mas_retriever(search_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m llm \u001b[38;5;241m=\u001b[39m Ollama(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/langchain_community/vectorstores/chroma.py:887\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    885\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    886\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m--> 887\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/langchain_community/vectorstores/chroma.py:817\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;28mcls\u001b[39m: Type[Chroma],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Chroma:\n\u001b[1;32m    798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a Chroma vectorstore from a raw documents.\u001b[39;00m\n\u001b[1;32m    799\u001b[0m \n\u001b[1;32m    800\u001b[0m \u001b[38;5;124;03m    If a persist_directory is specified, the collection will be persisted there.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;124;03m        Chroma: Chroma vectorstore.\u001b[39;00m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 817\u001b[0m     chroma_collection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m         ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(uuid\u001b[38;5;241m.\u001b[39muuid4()) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m texts]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:224\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     emit_warning()\n\u001b[0;32m--> 224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/langchain_community/vectorstores/chroma.py:86\u001b[0m, in \u001b[0;36mChroma.__init__\u001b[0;34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client, relevance_score_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import chromadb python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install chromadb`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     89\u001b[0m     )\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_settings \u001b[38;5;241m=\u001b[39m client_settings\n",
      "\u001b[0;31mImportError\u001b[0m: Could not import chromadb python package. Please install it with `pip install chromadb`."
     ]
    }
   ],
   "source": [
    "# From the previous notebook\n",
    "# --------------- LANGCHAIN + OLLAMA ---------------\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "loader = TextLoader(\"climate.txt\")\n",
    "raw_docs = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "docs = splitter.split_documents(raw_docs)\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "vs = Chroma.from_documents(docs, embeddings, persist_directory=\"lc_chroma\")\n",
    "retriever = vs.as_retriever(search_type=\"mmr\")\n",
    "\n",
    "llm = Ollama(model=\"llama3\")\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "print(\"[LangChain]\", qa.run(\"What causes the most CO2 emissions?\"))\n",
    "# --------------- LANGCHAIN + OLLAMA -------------------\n",
    "\n",
    "# Load the required LangChain components\n",
    "from langchain_community.llms import Ollama                    # Local LLM interface (e.g., llama3)\n",
    "from langchain.embeddings import OllamaEmbeddings              # Embedding model via Ollama (e.g., mxbai-embed-large)\n",
    "from langchain.vectorstores import Chroma                      # Vector store to store and search chunk embeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Splits long text into manageable chunks\n",
    "from langchain.document_loaders import TextLoader              # Utility to load text files as documents\n",
    "from langchain.chains import RetrievalQA                       # Retrieval-augmented QA pipeline\n",
    "\n",
    "# ------------------- Perception: Load and Prepare Knowledge -------------------\n",
    "\n",
    "# Load the raw document from file\n",
    "loader = TextLoader(\"climate.txt\")\n",
    "raw_docs = loader.load()  # List of Document objects\n",
    "\n",
    "# Split the document into smaller overlapping chunks for better semantic matching\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "docs = splitter.split_documents(raw_docs)\n",
    "\n",
    "# ------------------- Representation: Embed and Store Chunks -------------------\n",
    "\n",
    "# Load a local embedding model via Ollama (can be changed to any compatible embedding model)\n",
    "embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "\n",
    "# Create or load a Chroma vector store from the split chunks\n",
    "vs = Chroma.from_documents(docs, embeddings, persist_directory=\"lc_chroma\")\n",
    "\n",
    "# Create a retriever using Max Marginal Relevance (MMR) to reduce redundancy\n",
    "retriever = vs.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 4})\n",
    "\n",
    "# ------------------- Reasoning: Setup the LLM -------------------\n",
    "\n",
    "# Load the local language model (e.g., LLaMA 3 via Ollama)\n",
    "llm = Ollama(model=\"llama3\")\n",
    "\n",
    "# Wrap everything in a RetrievalQA chain: retrieve relevant chunk, pass to LLM, and get answer\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "\n",
    "# ------------------- Action: Ask a Question -------------------\n",
    "\n",
    "# Run the full agentic pipeline\n",
    "print(\"[LangChain]\", qa.invoke(\"What causes the most CO2 emissions?\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a4d7db-daeb-4dd1-b1e1-3b4987604147",
   "metadata": {},
   "source": [
    "This LangChain RAG setup behaves like an agent in a sandbox: it perceives its environment (a fixed document), chooses the most relevant information (via retrieval), and takes a single goal-driven action (answering the question). It’s not a fully autonomous agent — but it captures the core structure of one.\n",
    "\n",
    "Let's increase the agentic side, by leveraging a proper agent call, and providing to the agent the choice between several tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22add533-d24f-4805-ae77-92f73a4b4571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Agentic LangChain] The burning of fossil fuels for energy and transportation, including coal-fired power plants, gasoline-powered vehicles, and industrial processes like cement production.\n",
      "\n",
      "Reasoning: Throughout our investigation, we consistently found that the burning of fossil fuels is a significant contributor to CO2 emissions. Specifically, it was mentioned in both the local document and Wikipedia search results that this activity is the largest contributor to CO2 emissions. Additionally, the search result provided information on the historical increase in global temperatures driven by human activities, especially fossil fuel burning since the Industrial Revolution.\n",
      "\n",
      "Therefore, based on the evidence gathered through LocalDocQA and WikipediaSearch, it appears that the most significant cause of CO2 emissions is the burning of fossil fuels for energy and transportation.\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import Tool, initialize_agent\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.llms import Ollama\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# 1. Load and embed local document\n",
    "loader = TextLoader(\"climate.txt\")\n",
    "docs = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100).split_documents(loader.load())\n",
    "vs = Chroma.from_documents(docs, OllamaEmbeddings(model=\"mxbai-embed-large\"), persist_directory=\"lc_agent_chroma\")\n",
    "retriever = vs.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 4})\n",
    "\n",
    "# 2. Build QA chain on top of retriever\n",
    "llm = Ollama(model=\"llama3\")\n",
    "rag_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "\n",
    "# 3. Add Wikipedia as external tool\n",
    "wiki_tool = WikipediaAPIWrapper()\n",
    "\n",
    "# 4. Wrap both tools as LangChain Tool objects\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"LocalDocQA\",\n",
    "        func=rag_chain.run,\n",
    "        description=\"Use this tool to answer questions about climate change using the local document.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"WikipediaSearch\",\n",
    "        func=wiki_tool.run,\n",
    "        description=\"Use this when the local document does not seem to contain the information.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# 5. Initialize the agent\n",
    "agent_executor = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# 6. Run the agent\n",
    "response = agent_executor.run(\"What causes the most CO2 emissions?\")\n",
    "print(\"[Agentic LangChain]\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cb7274-2d2e-41b4-b596-d5c831f811b7",
   "metadata": {},
   "source": [
    "What happens in the background? Let's expose the call that the \"agent\" program makes, the responses of the LLM, and the loops that occur until the LLM surfaces the expected answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "311f07c4-1643-4f64-8578-4cd23ae79c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Prompt Template (ReAct Format with Placeholders) ===\n",
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "LocalDocQA(*args: Any, callbacks: Union[list[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = None, tags: Optional[list[str]] = None, metadata: Optional[dict[str, Any]] = None, **kwargs: Any) -> Any - Use this tool to answer questions about climate change using the local document.\n",
      "WikipediaSearch(query: str) -> str - Use this when the local document does not contain the needed information.\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [LocalDocQA, WikipediaSearch]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: {input}\n",
      "Thought:{agent_scratchpad}\n",
      "\n",
      "=== Rendered Prompt Sent to the LLM ===\n",
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "LocalDocQA(*args: Any, callbacks: Union[list[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = None, tags: Optional[list[str]] = None, metadata: Optional[dict[str, Any]] = None, **kwargs: Any) -> Any - Use this tool to answer questions about climate change using the local document.\n",
      "WikipediaSearch(query: str) -> str - Use this when the local document does not contain the needed information.\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [LocalDocQA, WikipediaSearch]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: What causes the most CO2 emissions?\n",
      "Thought:\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I'm not sure what causes the most CO2 emissions, but I should try to find out.\n",
      "\n",
      "Action: LocalDocQA\n",
      "Action Input: None (since we're starting from scratch)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mSince there is no question provided, I won't attempt to answer anything!\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mLet's continue!\n",
      "\n",
      "Thought: Hmm, that makes sense. Let me try again with a more specific question.\n",
      "\n",
      "Action: LocalDocQA\n",
      "Action Input: \"What are the main causes of CO2 emissions?\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe main causes of CO2 emissions are:\n",
      "\n",
      "* Burning of fossil fuels for energy and transportation (including coal-fired power plants, gasoline-powered vehicles, and industrial processes like cement production)\n",
      "* Deforestation\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mLet's continue!\n",
      "\n",
      "Thought: Okay, so I know that burning of fossil fuels and deforestation are two main causes of CO2 emissions. But I should also try to find out if there are any other significant contributors.\n",
      "\n",
      "Action: LocalDocQA\n",
      "Action Input: \"What are some additional factors that contribute to CO2 emissions?\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAccording to the provided context, deforestation also contributes to CO2 emissions by reducing the planet's ability to absorb CO2. It does not mention any other specific factors that contribute to CO2 emissions.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mLet's continue!\n",
      "\n",
      "Thought: Okay, so I know that burning of fossil fuels and deforestation are two main causes of CO2 emissions, and there might be some additional factors contributing as well. But what about the rest of the story? Are there any other significant contributors to CO2 emissions that aren't mentioned?\n",
      "\n",
      "Action: WikipediaSearch\n",
      "Action Input: \"What are some other significant contributors to CO2 emissions?\"\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mPage: Climate change\n",
      "Summary: Present-day climate change includes both global warming—the ongoing increase in global average temperature—and its wider effects on Earth's climate system. Climate change in a broader sense also includes previous long-term changes to Earth's climate. The current rise in global temperatures is driven by human activities, especially fossil fuel burning since the Industrial Revolution. Fossil fuel use, deforestation, and some agricultural and industrial practices release greenhouse gases. These gases absorb some of the heat that the Earth radiates after it warms from sunlight, warming the lower atmosphere. Carbon dioxide, the primary gas driving global warming, has increased in concentration by about 50% since the pre-industrial era to levels not seen for millions of years.\n",
      "Climate change has an increasingly large impact on the environment. Deserts are expanding, while heat waves and wildfires are becoming more common. Amplified warming in the Arctic has contributed to thawing permafrost, retreat of glaciers and sea ice decline. Higher temperatures are also causing more intense storms, droughts, and other weather extremes. Rapid environmental change in mountains, coral reefs, and the Arctic is forcing many species to relocate or become extinct. Even if efforts to minimize future warming are successful, some effects will continue for centuries. These include ocean heating, ocean acidification and sea level rise.\n",
      "Climate change threatens people with increased flooding, extreme heat, increased food and water scarcity, more disease, and economic loss. Human migration and conflict can also be a result. The World Health Organization calls climate change one of the biggest threats to global health in the 21st century. Societies and ecosystems will experience more severe risks without action to limit warming. Adapting to climate change through efforts like flood control measures or drought-resistant crops partially reduces climate change risks, although some limits to adaptation have already been reached. Poorer communities are responsible for a small share of global emissions, yet have the least ability to adapt and are most vulnerable to climate change.\n",
      "\n",
      "Many climate change impacts have been observed in the first decades of the 21st century, with 2024 the warmest on record at +1.60 °C (2.88 °F) since regular tracking began in 1850. Additional warming will increase these impacts and can trigger tipping points, such as melting all of the Greenland ice sheet. Under the 2015 Paris Agreement, nations collectively agreed to keep warming \"well under 2 °C\". However, with pledges made under the Agreement, global warming would still reach about 2.8 °C (5.0 °F) by the end of the century. Limiting warming to 1.5 °C would require halving emissions by 2030 and achieving net-zero emissions by 2050.\n",
      "There is widespread support for climate action worldwide. Fossil fuel use can be phased out by conserving energy and switching to energy sources that do not produce significant carbon pollution. These energy sources include wind, solar, hydro, and nuclear power. Cleanly generated electricity can replace fossil fuels for powering transportation, heating buildings, and running industrial processes. Carbon can also be removed from the atmosphere, for instance by increasing forest cover and farming with methods that capture carbon in soil.\n",
      "\n",
      "Page: Greenhouse gas emissions\n",
      "Summary: Greenhouse gas (GHG) emissions from human activities intensify the greenhouse effect. This contributes to climate change. Carbon dioxide (CO2), from burning fossil fuels such as coal, oil, and natural gas, is the main cause of climate change. The largest annual emissions are from China followed by the United States. The United States has higher emissions per capita. The main producers fueling the emissions globally are large oil and gas companies. Emissions from human activities have increased atmospheric carbon dioxide by about 50% over pre-industrial le\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI can see that you've been exploring the topic of CO2 emissions! Let's summarize what we've learned so far:\n",
      "\n",
      "* Burning of fossil fuels for energy and transportation, as well as deforestation, are two main causes of CO2 emissions.\n",
      "* There might be some additional factors contributing to CO2 emissions, but they haven't been specifically mentioned.\n",
      "\n",
      "Now, let's consider the results from our Wikipedia search. It looks like we've gained a lot more information on the topic!\n",
      "\n",
      "From what I can see, it seems that:\n",
      "\n",
      "* Human activities, such as fossil fuel burning and deforestation, are driving climate change.\n",
      "* Climate change has significant impacts on the environment, including rising temperatures, sea-level rise, and changes in weather patterns.\n",
      "* It also poses risks to human health, food security, and economic stability.\n",
      "\n",
      "We've also learned about the importance of reducing emissions to limit warming to 1.5°C or below. This can be achieved by transitioning to cleaner energy sources, increasing forest cover, and adopting sustainable land-use practices.\n",
      "\n",
      "Now that we have a better understanding of the topic, let's think about our original question: \"What causes the most CO2 emissions?\"\n",
      "\n",
      "Thought: I now know the final answer\n",
      "\n",
      "Final Answer: The main causes of CO2 emissions are burning of fossil fuels for energy and transportation (including coal-fired power plants, gasoline-powered vehicles, and industrial processes like cement production), followed by deforestation.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "[Agentic LangChain] The main causes of CO2 emissions are burning of fossil fuels for energy and transportation (including coal-fired power plants, gasoline-powered vehicles, and industrial processes like cement production), followed by deforestation.\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import Tool, initialize_agent\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.llms import Ollama\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# Load and prepare documents\n",
    "loader = TextLoader(\"climate.txt\")\n",
    "docs = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100).split_documents(loader.load())\n",
    "vs = Chroma.from_documents(docs, OllamaEmbeddings(model=\"mxbai-embed-large\"), persist_directory=\"lc_agent_chroma\")\n",
    "retriever = vs.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 4})\n",
    "llm = Ollama(model=\"llama3\")\n",
    "\n",
    "# Define tools\n",
    "rag_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "wiki_tool = WikipediaAPIWrapper()\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"LocalDocQA\",\n",
    "        func=rag_chain.run,\n",
    "        description=\"Use this tool to answer questions about climate change using the local document.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"WikipediaSearch\",\n",
    "        func=wiki_tool.run,\n",
    "        description=\"Use this when the local document does not contain the needed information.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Initialize the agent\n",
    "agent_executor = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Insert this to reveal the internal LLM prompt\n",
    "from langchain.agents import AgentOutputParser\n",
    "\n",
    "# Show the base ReAct-style prompt template\n",
    "print(\"=== Prompt Template (ReAct Format with Placeholders) ===\")\n",
    "print(agent_executor.agent.llm_chain.prompt.template)\n",
    "\n",
    "# Show the full rendered prompt as it would be sent to the LLM\n",
    "formatted_prompt = agent_executor.agent.llm_chain.prompt.format_prompt(\n",
    "    input=\"What causes the most CO2 emissions?\",\n",
    "    tools=tools,\n",
    "    tool_names=[tool.name for tool in tools],\n",
    "    agent_scratchpad=\"\"  # This avoids the KeyError\n",
    ")\n",
    "print(\"\\n=== Rendered Prompt Sent to the LLM ===\")\n",
    "print(formatted_prompt.to_string())\n",
    "\n",
    "\n",
    "# Run the agent\n",
    "response = agent_executor.run(\"What causes the most CO2 emissions?\")\n",
    "print(\"[Agentic LangChain]\", response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d26694-461f-4fa5-b4a6-46cf7eacc1aa",
   "metadata": {},
   "source": [
    "## Building a Custom Agent with LangGraph (Step-by-Step)\n",
    "\n",
    "This section of the notebook demonstrates how to build a **simple, explainable agent** using **LangGraph**, mimicking the behavior of a LangChain `ZERO_SHOT_REACT_DESCRIPTION` agent — but with full control over its logic, flow, and execution.\n",
    "\n",
    "The effect is the same as the previous example, so you are on familiar ground, but the interest is that LangGraph allows you to build your own agent and its structure, so you can go beyond pre-canned tools.\n",
    "---\n",
    "\n",
    "##  Step 1: Set Up Tools\n",
    "\n",
    "This step loads and prepares everything needed to power the agent's capabilities.\n",
    "\n",
    "**Functions and Components Used:**\n",
    "- `TextLoader`: Loads the `climate.txt` document from disk.\n",
    "- `RecursiveCharacterTextSplitter`: Splits the document into overlapping chunks for better semantic search.\n",
    "- `OllamaEmbeddings`: Creates vector embeddings using a local model (e.g., `mxbai-embed-large`).\n",
    "- `Chroma`: Stores those embeddings for efficient retrieval.\n",
    "- `Ollama`: Runs the local language model (`llama3`) used for answering.\n",
    "- `RetrievalQA`: A wrapper that connects the retriever and the LLM.\n",
    "- `WikipediaAPIWrapper`: A tool that performs a Wikipedia search as a fallback.\n",
    "\n",
    " **Purpose:** These tools form the basis for all reasoning and information retrieval used later by the agent.\n",
    "\n",
    "\n",
    "##  Step 2: Define the Agent's State\n",
    "\n",
    "LangGraph requires a formal declaration of what data flows through your graph. This step defines that shared memory.\n",
    "\n",
    "**Structure: `QAState`**\n",
    "- `question`: The user’s query.\n",
    "- `rag_answer`: Stores the answer if the RAG tool was used.\n",
    "- `wiki_answer`: Stores the answer if the Wikipedia tool was used.\n",
    "- `answer`: The final answer to return. This key is marked with `Annotated[..., \"output\"]` so LangGraph knows to treat it as a **single-write endpoint**.\n",
    "\n",
    " **Purpose:** Controls the input, intermediate, and output values handled by each node in the graph.\n",
    "\n",
    "\n",
    "##  Step 3: Routing Logic\n",
    "\n",
    "This is the \"agentic brain\" — a custom Python function that decides *which tool to use* based on the question.\n",
    "\n",
    "**Function: `tool_selector(state)`**\n",
    "- Inspects `state[\"question\"]`\n",
    "- If the question contains the word `\"climate\"`, it returns `{\"__next__\": \"rag\"}`\n",
    "- Otherwise, it returns `{\"__next__\": \"wiki\"}`\n",
    "\n",
    " **Purpose:** Simulates an intelligent decision-maker — your own custom routing logic instead of relying on a hardcoded or LLM-parsed policy.\n",
    "\n",
    "\n",
    "##  Step 4: Define Graph Nodes\n",
    "\n",
    "Each node is a callable function that:\n",
    "- Accepts the current state\n",
    "- Returns a dictionary with updated values\n",
    "\n",
    "**Node Functions:**\n",
    "- `rag_node(state)`: Calls the RAG tool and stores the output in `rag_answer`.\n",
    "- `wiki_node(state)`: Calls Wikipedia and stores the output in `wiki_answer`.\n",
    "- `merger(state)`: Consolidates the answers. If `rag_answer` is present, it uses it. Otherwise, it falls back to `wiki_answer`. The result is stored in `answer`.\n",
    "\n",
    " **Purpose:** These nodes act like \"agent limbs\" — executing concrete actions depending on what the brain (`tool_selector`) decided.\n",
    "\n",
    "\n",
    "##  Step 5: Build the Graph\n",
    "\n",
    "Here we assemble the agent as a directed graph of reasoning steps.\n",
    "\n",
    "**Steps:**\n",
    "- Instantiate the graph: `graph = StateGraph(QAState)`\n",
    "- Add nodes: router, rag, wiki, merger\n",
    "- Connect edges:\n",
    "  - `router → rag`\n",
    "  - `router → wiki`\n",
    "  - `rag → merger`\n",
    "  - `wiki → merger`\n",
    "  - `merger → END`\n",
    "- Compile the graph: `agent = graph.compile()`\n",
    "\n",
    " **Purpose:** This defines the possible paths an input can take — making the reasoning traceable and testable.\n",
    "\n",
    "\n",
    "##  Step 6: Run the Agent\n",
    "\n",
    "We finally invoke the agent with a natural language question.\n",
    "\n",
    "**Invocation:**\n",
    "```python\n",
    "question = \"What causes the most CO2 emissions?\"\n",
    "result = agent.invoke({\"question\": question})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa4d202c-a236-4420-ac0e-7ed88490122e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LangGraph Agentic] The largest contributor to CO2 emissions is the burning of fossil fuels for energy and transportation. This includes coal-fired power plants, gasoline-powered vehicles, and industrial processes like cement production.\n"
     ]
    }
   ],
   "source": [
    "# ---------------- LangGraph Agentic Example ----------------\n",
    "\n",
    "from typing import TypedDict, Annotated, Optional, Union\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# === Step 1: Set up tools ===\n",
    "docs = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100).split_documents(\n",
    "    TextLoader(\"climate.txt\").load()\n",
    ")\n",
    "vs = Chroma.from_documents(docs, OllamaEmbeddings(model=\"mxbai-embed-large\"), persist_directory=\"graph_chroma\")\n",
    "retriever = vs.as_retriever()\n",
    "llm = Ollama(model=\"llama3\")\n",
    "rag_tool = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "wiki_tool = WikipediaAPIWrapper()\n",
    "\n",
    "# === Step 2: Define state ===\n",
    "class QAState(TypedDict):\n",
    "    question: str\n",
    "    rag_answer: Optional[str]\n",
    "    wiki_answer: Optional[str]\n",
    "    answer: Annotated[Optional[str], \"output\"]  # Final output goes here\n",
    "\n",
    "# === Step 3: Routing logic ===\n",
    "def tool_selector(state: QAState):\n",
    "    if \"climate\" in state[\"question\"].lower():\n",
    "        return {\"__next__\": \"rag\"}\n",
    "    else:\n",
    "        return {\"__next__\": \"wiki\"}\n",
    "\n",
    "# === Step 4: Nodes ===\n",
    "def rag_node(state: QAState):\n",
    "    return {\"rag_answer\": rag_tool.run(state[\"question\"])}\n",
    "\n",
    "def wiki_node(state: QAState):\n",
    "    return {\"wiki_answer\": wiki_tool.run(state[\"question\"])}\n",
    "\n",
    "def merger(state: QAState):\n",
    "    return {\"answer\": state.get(\"rag_answer\") or state.get(\"wiki_answer\")}\n",
    "\n",
    "# === Step 5: Build graph ===\n",
    "graph = StateGraph(QAState)\n",
    "graph.add_node(\"router\", RunnableLambda(tool_selector))\n",
    "graph.add_node(\"rag\", rag_node)\n",
    "graph.add_node(\"wiki\", wiki_node)\n",
    "graph.add_node(\"merger\", merger)\n",
    "\n",
    "graph.set_entry_point(\"router\")\n",
    "graph.add_edge(\"router\", \"rag\")\n",
    "graph.add_edge(\"router\", \"wiki\")\n",
    "graph.add_edge(\"rag\", \"merger\")\n",
    "graph.add_edge(\"wiki\", \"merger\")\n",
    "graph.add_edge(\"merger\", END)\n",
    "\n",
    "agent = graph.compile()\n",
    "\n",
    "# === Step 6: Run it ===\n",
    "question = \"What causes the most CO2 emissions?\"\n",
    "result = agent.invoke({\"question\": question})\n",
    "print(\"[LangGraph Agentic]\", result[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8233b589-676c-4975-b219-4bb50d226a6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
