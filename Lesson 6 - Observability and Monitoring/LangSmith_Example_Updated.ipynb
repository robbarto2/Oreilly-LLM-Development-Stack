{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7396e6-923e-4e9c-8dd6-73c433657e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the previous notebook\n",
    "# --------------- LANGCHAIN + OLLAMA ---------------\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "loader = TextLoader(\"climate.txt\")\n",
    "raw_docs = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "docs = splitter.split_documents(raw_docs)\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "vs = Chroma.from_documents(docs, embeddings, persist_directory=\"lc_chroma\")\n",
    "retriever = vs.as_retriever(search_type=\"mmr\")\n",
    "\n",
    "llm = Ollama(model=\"llama3\")\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "print(\"[LangChain]\", qa.run(\"What causes the most CO2 emissions?\"))\n",
    "# --------------- LANGCHAIN + OLLAMA -------------------\n",
    "\n",
    "# Load the required LangChain components\n",
    "from langchain_community.llms import Ollama                    # Local LLM interface (e.g., llama3)\n",
    "from langchain.embeddings import OllamaEmbeddings              # Embedding model via Ollama (e.g., mxbai-embed-large)\n",
    "from langchain.vectorstores import Chroma                      # Vector store to store and search chunk embeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Splits long text into manageable chunks\n",
    "from langchain.document_loaders import TextLoader              # Utility to load text files as documents\n",
    "from langchain.chains import RetrievalQA                       # Retrieval-augmented QA pipeline\n",
    "\n",
    "# ------------------- Perception: Load and Prepare Knowledge -------------------\n",
    "\n",
    "# Load the raw document from file\n",
    "loader = TextLoader(\"climate.txt\")\n",
    "raw_docs = loader.load()  # List of Document objects\n",
    "\n",
    "# Split the document into smaller overlapping chunks for better semantic matching\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "docs = splitter.split_documents(raw_docs)\n",
    "\n",
    "# ------------------- Representation: Embed and Store Chunks -------------------\n",
    "\n",
    "# Load a local embedding model via Ollama (can be changed to any compatible embedding model)\n",
    "embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "\n",
    "# Create or load a Chroma vector store from the split chunks\n",
    "vs = Chroma.from_documents(docs, embeddings, persist_directory=\"lc_chroma\")\n",
    "\n",
    "# Create a retriever using Max Marginal Relevance (MMR) to reduce redundancy\n",
    "retriever = vs.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 4})\n",
    "\n",
    "# ------------------- Reasoning: Setup the LLM -------------------\n",
    "\n",
    "# Load the local language model (e.g., LLaMA 3 via Ollama)\n",
    "llm = Ollama(model=\"llama3\")\n",
    "\n",
    "# Wrap everything in a RetrievalQA chain: retrieve relevant chunk, pass to LLM, and get answer\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "\n",
    "# ------------------- Action: Ask a Question -------------------\n",
    "\n",
    "# Run the full agentic pipeline\n",
    "print(\"[LangChain]\", qa.invoke(\"What causes the most CO2 emissions?\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a4d7db-daeb-4dd1-b1e1-3b4987604147",
   "metadata": {},
   "source": [
    "This LangChain RAG setup behaves like an agent in a sandbox: it perceives its environment (a fixed document), chooses the most relevant information (via retrieval), and takes a single goal-driven action (answering the question). It’s not a fully autonomous agent — but it captures the core structure of one.\n",
    "\n",
    "Let's increase the agentic side, by leveraging a proper agent call, and providing to the agent the choice between several tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22add533-d24f-4805-ae77-92f73a4b4571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool, initialize_agent\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.llms import Ollama\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# 1. Load and embed local document\n",
    "loader = TextLoader(\"climate.txt\")\n",
    "docs = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100).split_documents(loader.load())\n",
    "vs = Chroma.from_documents(docs, OllamaEmbeddings(model=\"mxbai-embed-large\"), persist_directory=\"lc_agent_chroma\")\n",
    "retriever = vs.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 4})\n",
    "\n",
    "# 2. Build QA chain on top of retriever\n",
    "llm = Ollama(model=\"llama3\")\n",
    "rag_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "\n",
    "# 3. Add Wikipedia as external tool\n",
    "wiki_tool = WikipediaAPIWrapper()\n",
    "\n",
    "# 4. Wrap both tools as LangChain Tool objects\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"LocalDocQA\",\n",
    "        func=rag_chain.run,\n",
    "        description=\"Use this tool to answer questions about climate change using the local document.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"WikipediaSearch\",\n",
    "        func=wiki_tool.run,\n",
    "        description=\"Use this when the local document does not seem to contain the information.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# 5. Initialize the agent\n",
    "agent_executor = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# 6. Run the agent\n",
    "response = agent_executor.run(\"What causes the most CO2 emissions?\")\n",
    "print(\"[Agentic LangChain]\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cb7274-2d2e-41b4-b596-d5c831f811b7",
   "metadata": {},
   "source": [
    "What happens in the background? Let's expose the call that the \"agent\" program makes, the responses of the LLM, and the loops that occur until the LLM surfaces the expected answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311f07c4-1643-4f64-8578-4cd23ae79c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool, initialize_agent\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.llms import Ollama\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# Load and prepare documents\n",
    "loader = TextLoader(\"climate.txt\")\n",
    "docs = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100).split_documents(loader.load())\n",
    "vs = Chroma.from_documents(docs, OllamaEmbeddings(model=\"mxbai-embed-large\"), persist_directory=\"lc_agent_chroma\")\n",
    "retriever = vs.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 4})\n",
    "llm = Ollama(model=\"llama3\")\n",
    "\n",
    "# Define tools\n",
    "rag_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "wiki_tool = WikipediaAPIWrapper()\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"LocalDocQA\",\n",
    "        func=rag_chain.run,\n",
    "        description=\"Use this tool to answer questions about climate change using the local document.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"WikipediaSearch\",\n",
    "        func=wiki_tool.run,\n",
    "        description=\"Use this when the local document does not contain the needed information.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Initialize the agent\n",
    "agent_executor = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Insert this to reveal the internal LLM prompt\n",
    "from langchain.agents import AgentOutputParser\n",
    "\n",
    "# Show the base ReAct-style prompt template\n",
    "print(\"=== Prompt Template (ReAct Format with Placeholders) ===\")\n",
    "print(agent_executor.agent.llm_chain.prompt.template)\n",
    "\n",
    "# Show the full rendered prompt as it would be sent to the LLM\n",
    "formatted_prompt = agent_executor.agent.llm_chain.prompt.format_prompt(\n",
    "    input=\"What causes the most CO2 emissions?\",\n",
    "    tools=tools,\n",
    "    tool_names=[tool.name for tool in tools],\n",
    "    agent_scratchpad=\"\"  # This avoids the KeyError\n",
    ")\n",
    "print(\"\\n=== Rendered Prompt Sent to the LLM ===\")\n",
    "print(formatted_prompt.to_string())\n",
    "\n",
    "\n",
    "# Run the agent\n",
    "response = agent_executor.run(\"What causes the most CO2 emissions?\")\n",
    "print(\"[Agentic LangChain]\", response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405aad46",
   "metadata": {},
   "source": [
    "\n",
    "## 🧠 LangSmith Integration\n",
    "\n",
    "The following cells enable LangSmith observability for this notebook. We configure the environment,\n",
    "wrap key functions with `@traceable`, and route calls through LangSmith so traces appear in your dashboard.\n",
    "\n",
    "👉 You need a LangSmith account and an API key from https://smith.langchain.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488df8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# Enable LangSmith tracing\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"your-langsmith-api-key\"  # Replace with your actual key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e751bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langsmith.trace import traceable\n",
    "\n",
    "@traceable(name=\"RAG Chain: Climate CO2 QA\")\n",
    "def run_rag(query):\n",
    "    return qa.run(query)\n",
    "\n",
    "@traceable(name=\"Agent Execution: Tool-augmented QA\")\n",
    "def run_agent(query):\n",
    "    return agent_executor.run(query)\n",
    "\n",
    "# Run the agent and trace the call\n",
    "response = run_agent(\"What causes the most CO2 emissions?\")\n",
    "print(\"[LangSmith Traced Agent]\", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d26694-461f-4fa5-b4a6-46cf7eacc1aa",
   "metadata": {},
   "source": [
    "## Building a Custom Agent with LangGraph (Step-by-Step)\n",
    "\n",
    "This section of the notebook demonstrates how to build a **simple, explainable agent** using **LangGraph**, mimicking the behavior of a LangChain `ZERO_SHOT_REACT_DESCRIPTION` agent — but with full control over its logic, flow, and execution.\n",
    "\n",
    "The effect is the same as the previous example, so you are on familiar ground, but the interest is that LangGraph allows you to build your own agent and its structure, so you can go beyond pre-canned tools.\n",
    "---\n",
    "\n",
    "##  Step 1: Set Up Tools\n",
    "\n",
    "This step loads and prepares everything needed to power the agent's capabilities.\n",
    "\n",
    "**Functions and Components Used:**\n",
    "- `TextLoader`: Loads the `climate.txt` document from disk.\n",
    "- `RecursiveCharacterTextSplitter`: Splits the document into overlapping chunks for better semantic search.\n",
    "- `OllamaEmbeddings`: Creates vector embeddings using a local model (e.g., `mxbai-embed-large`).\n",
    "- `Chroma`: Stores those embeddings for efficient retrieval.\n",
    "- `Ollama`: Runs the local language model (`llama3`) used for answering.\n",
    "- `RetrievalQA`: A wrapper that connects the retriever and the LLM.\n",
    "- `WikipediaAPIWrapper`: A tool that performs a Wikipedia search as a fallback.\n",
    "\n",
    " **Purpose:** These tools form the basis for all reasoning and information retrieval used later by the agent.\n",
    "\n",
    "\n",
    "##  Step 2: Define the Agent's State\n",
    "\n",
    "LangGraph requires a formal declaration of what data flows through your graph. This step defines that shared memory.\n",
    "\n",
    "**Structure: `QAState`**\n",
    "- `question`: The user’s query.\n",
    "- `rag_answer`: Stores the answer if the RAG tool was used.\n",
    "- `wiki_answer`: Stores the answer if the Wikipedia tool was used.\n",
    "- `answer`: The final answer to return. This key is marked with `Annotated[..., \"output\"]` so LangGraph knows to treat it as a **single-write endpoint**.\n",
    "\n",
    " **Purpose:** Controls the input, intermediate, and output values handled by each node in the graph.\n",
    "\n",
    "\n",
    "##  Step 3: Routing Logic\n",
    "\n",
    "This is the \"agentic brain\" — a custom Python function that decides *which tool to use* based on the question.\n",
    "\n",
    "**Function: `tool_selector(state)`**\n",
    "- Inspects `state[\"question\"]`\n",
    "- If the question contains the word `\"climate\"`, it returns `{\"__next__\": \"rag\"}`\n",
    "- Otherwise, it returns `{\"__next__\": \"wiki\"}`\n",
    "\n",
    " **Purpose:** Simulates an intelligent decision-maker — your own custom routing logic instead of relying on a hardcoded or LLM-parsed policy.\n",
    "\n",
    "\n",
    "##  Step 4: Define Graph Nodes\n",
    "\n",
    "Each node is a callable function that:\n",
    "- Accepts the current state\n",
    "- Returns a dictionary with updated values\n",
    "\n",
    "**Node Functions:**\n",
    "- `rag_node(state)`: Calls the RAG tool and stores the output in `rag_answer`.\n",
    "- `wiki_node(state)`: Calls Wikipedia and stores the output in `wiki_answer`.\n",
    "- `merger(state)`: Consolidates the answers. If `rag_answer` is present, it uses it. Otherwise, it falls back to `wiki_answer`. The result is stored in `answer`.\n",
    "\n",
    " **Purpose:** These nodes act like \"agent limbs\" — executing concrete actions depending on what the brain (`tool_selector`) decided.\n",
    "\n",
    "\n",
    "##  Step 5: Build the Graph\n",
    "\n",
    "Here we assemble the agent as a directed graph of reasoning steps.\n",
    "\n",
    "**Steps:**\n",
    "- Instantiate the graph: `graph = StateGraph(QAState)`\n",
    "- Add nodes: router, rag, wiki, merger\n",
    "- Connect edges:\n",
    "  - `router → rag`\n",
    "  - `router → wiki`\n",
    "  - `rag → merger`\n",
    "  - `wiki → merger`\n",
    "  - `merger → END`\n",
    "- Compile the graph: `agent = graph.compile()`\n",
    "\n",
    " **Purpose:** This defines the possible paths an input can take — making the reasoning traceable and testable.\n",
    "\n",
    "\n",
    "##  Step 6: Run the Agent\n",
    "\n",
    "We finally invoke the agent with a natural language question.\n",
    "\n",
    "**Invocation:**\n",
    "```python\n",
    "question = \"What causes the most CO2 emissions?\"\n",
    "result = agent.invoke({\"question\": question})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4d202c-a236-4420-ac0e-7ed88490122e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- LangGraph Agentic Example ----------------\n",
    "\n",
    "from typing import TypedDict, Annotated, Optional, Union\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# === Step 1: Set up tools ===\n",
    "docs = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100).split_documents(\n",
    "    TextLoader(\"climate.txt\").load()\n",
    ")\n",
    "vs = Chroma.from_documents(docs, OllamaEmbeddings(model=\"mxbai-embed-large\"), persist_directory=\"graph_chroma\")\n",
    "retriever = vs.as_retriever()\n",
    "llm = Ollama(model=\"llama3\")\n",
    "rag_tool = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "wiki_tool = WikipediaAPIWrapper()\n",
    "\n",
    "# === Step 2: Define state ===\n",
    "class QAState(TypedDict):\n",
    "    question: str\n",
    "    rag_answer: Optional[str]\n",
    "    wiki_answer: Optional[str]\n",
    "    answer: Annotated[Optional[str], \"output\"]  # Final output goes here\n",
    "\n",
    "# === Step 3: Routing logic ===\n",
    "def tool_selector(state: QAState):\n",
    "    if \"climate\" in state[\"question\"].lower():\n",
    "        return {\"__next__\": \"rag\"}\n",
    "    else:\n",
    "        return {\"__next__\": \"wiki\"}\n",
    "\n",
    "# === Step 4: Nodes ===\n",
    "def rag_node(state: QAState):\n",
    "    return {\"rag_answer\": rag_tool.run(state[\"question\"])}\n",
    "\n",
    "def wiki_node(state: QAState):\n",
    "    return {\"wiki_answer\": wiki_tool.run(state[\"question\"])}\n",
    "\n",
    "def merger(state: QAState):\n",
    "    return {\"answer\": state.get(\"rag_answer\") or state.get(\"wiki_answer\")}\n",
    "\n",
    "# === Step 5: Build graph ===\n",
    "graph = StateGraph(QAState)\n",
    "graph.add_node(\"router\", RunnableLambda(tool_selector))\n",
    "graph.add_node(\"rag\", rag_node)\n",
    "graph.add_node(\"wiki\", wiki_node)\n",
    "graph.add_node(\"merger\", merger)\n",
    "\n",
    "graph.set_entry_point(\"router\")\n",
    "graph.add_edge(\"router\", \"rag\")\n",
    "graph.add_edge(\"router\", \"wiki\")\n",
    "graph.add_edge(\"rag\", \"merger\")\n",
    "graph.add_edge(\"wiki\", \"merger\")\n",
    "graph.add_edge(\"merger\", END)\n",
    "\n",
    "agent = graph.compile()\n",
    "\n",
    "# === Step 6: Run it ===\n",
    "question = \"What causes the most CO2 emissions?\"\n",
    "result = agent.invoke({\"question\": question})\n",
    "print(\"[LangGraph Agentic]\", result[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8233b589-676c-4975-b219-4bb50d226a6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM Stack)",
   "language": "python",
   "name": "llm-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
