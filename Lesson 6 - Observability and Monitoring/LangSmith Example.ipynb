{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc7396e6-923e-4e9c-8dd6-73c433657e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wf/jmbr4tf11xj6_b2633x2y_8m0000gn/T/ipykernel_33972/597022834.py:15: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
      "/var/folders/wf/jmbr4tf11xj6_b2633x2y_8m0000gn/T/ipykernel_33972/597022834.py:19: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"llama3\")\n",
      "/var/folders/wf/jmbr4tf11xj6_b2633x2y_8m0000gn/T/ipykernel_33972/597022834.py:21: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(\"[LangChain]\", qa.run(\"What causes the most CO2 emissions?\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LangChain] The largest contributor to CO2 emissions is the burning of fossil fuels for energy and transportation. This includes coal-fired power plants, gasoline-powered vehicles, and industrial processes like cement production.\n",
      "[LangChain] {'query': 'What causes the most CO2 emissions?', 'result': \"The largest contributor to CO2 emissions is the burning of fossil fuels for energy and transportation. This includes coal-fired power plants, gasoline-powered vehicles, and industrial processes like cement production. Deforestation also contributes by reducing the planet's ability to absorb CO2.\"}\n"
     ]
    }
   ],
   "source": [
    "# From the previous notebook\n",
    "# --------------- LANGCHAIN + OLLAMA ---------------\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "loader = TextLoader(\"climate.txt\")\n",
    "raw_docs = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "docs = splitter.split_documents(raw_docs)\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "vs = Chroma.from_documents(docs, embeddings, persist_directory=\"lc_chroma\")\n",
    "retriever = vs.as_retriever(search_type=\"mmr\")\n",
    "\n",
    "llm = Ollama(model=\"llama3\")\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "print(\"[LangChain]\", qa.run(\"What causes the most CO2 emissions?\"))\n",
    "# --------------- LANGCHAIN + OLLAMA -------------------\n",
    "\n",
    "# Load the required LangChain components\n",
    "from langchain_community.llms import Ollama                    # Local LLM interface (e.g., llama3)\n",
    "from langchain.embeddings import OllamaEmbeddings              # Embedding model via Ollama (e.g., mxbai-embed-large)\n",
    "from langchain.vectorstores import Chroma                      # Vector store to store and search chunk embeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Splits long text into manageable chunks\n",
    "from langchain.document_loaders import TextLoader              # Utility to load text files as documents\n",
    "from langchain.chains import RetrievalQA                       # Retrieval-augmented QA pipeline\n",
    "\n",
    "# ------------------- Perception: Load and Prepare Knowledge -------------------\n",
    "\n",
    "# Load the raw document from file\n",
    "loader = TextLoader(\"climate.txt\")\n",
    "raw_docs = loader.load()  # List of Document objects\n",
    "\n",
    "# Split the document into smaller overlapping chunks for better semantic matching\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "docs = splitter.split_documents(raw_docs)\n",
    "\n",
    "# ------------------- Representation: Embed and Store Chunks -------------------\n",
    "\n",
    "# Load a local embedding model via Ollama (can be changed to any compatible embedding model)\n",
    "embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "\n",
    "# Create or load a Chroma vector store from the split chunks\n",
    "vs = Chroma.from_documents(docs, embeddings, persist_directory=\"lc_chroma\")\n",
    "\n",
    "# Create a retriever using Max Marginal Relevance (MMR) to reduce redundancy\n",
    "retriever = vs.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 4})\n",
    "\n",
    "# ------------------- Reasoning: Setup the LLM -------------------\n",
    "\n",
    "# Load the local language model (e.g., LLaMA 3 via Ollama)\n",
    "llm = Ollama(model=\"llama3\")\n",
    "\n",
    "# Wrap everything in a RetrievalQA chain: retrieve relevant chunk, pass to LLM, and get answer\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "\n",
    "# ------------------- Action: Ask a Question -------------------\n",
    "\n",
    "# Run the full agentic pipeline\n",
    "print(\"[LangChain]\", qa.invoke(\"What causes the most CO2 emissions?\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a4d7db-daeb-4dd1-b1e1-3b4987604147",
   "metadata": {},
   "source": [
    "This LangChain RAG setup behaves like an agent in a sandbox: it perceives its environment (a fixed document), chooses the most relevant information (via retrieval), and takes a single goal-driven action (answering the question). It’s not a fully autonomous agent — but it captures the core structure of one.\n",
    "\n",
    "Let's increase the agentic side, by leveraging a proper agent call, and providing to the agent the choice between several tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22add533-d24f-4805-ae77-92f73a4b4571",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wf/jmbr4tf11xj6_b2633x2y_8m0000gn/T/ipykernel_33972/787049259.py:39: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent_executor = initialize_agent(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: `Thought: It seems like I've gathered a lot of information about what causes the most CO2 emissions and how different sectors contribute to those emissions.\n\nAction: None needed this time!\n`\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[0;32m~/Programming Projects/Pearson Live Training Repos/Oreilly-LLM-Development-Stack/venv/lib/python3.10/site-packages/langchain/agents/agent.py:1354\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[0;32m-> 1354\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_action_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1357\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Programming Projects/Pearson Live Training Repos/Oreilly-LLM-Development-Stack/venv/lib/python3.10/site-packages/langchain/agents/agent.py:801\u001b[0m, in \u001b[0;36mAgent.plan\u001b[0;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    800\u001b[0m full_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mpredict(callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfull_inputs)\n\u001b[0;32m--> 801\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming Projects/Pearson Live Training Repos/Oreilly-LLM-Development-Stack/venv/lib/python3.10/site-packages/langchain/agents/mrkl/output_parser.py:88\u001b[0m, in \u001b[0;36mMRKLOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]*Action\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*Input\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*:[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]*(.*)\u001b[39m\u001b[38;5;124m\"\u001b[39m, text, re\u001b[38;5;241m.\u001b[39mDOTALL\n\u001b[1;32m     87\u001b[0m ):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not parse LLM output: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     90\u001b[0m         observation\u001b[38;5;241m=\u001b[39mMISSING_ACTION_INPUT_AFTER_ACTION_ERROR_MESSAGE,\n\u001b[1;32m     91\u001b[0m         llm_output\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m     92\u001b[0m         send_to_llm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     93\u001b[0m     )\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Could not parse LLM output: `Thought: It seems like I've gathered a lot of information about what causes the most CO2 emissions and how different sectors contribute to those emissions.\n\nAction: None needed this time!\n`\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 47\u001b[0m\n\u001b[1;32m     39\u001b[0m agent_executor \u001b[38;5;241m=\u001b[39m initialize_agent(\n\u001b[1;32m     40\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[1;32m     41\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m     42\u001b[0m     agent\u001b[38;5;241m=\u001b[39mAgentType\u001b[38;5;241m.\u001b[39mZERO_SHOT_REACT_DESCRIPTION,\n\u001b[1;32m     43\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# 6. Run the agent\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat causes the most CO2 emissions?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Agentic LangChain]\u001b[39m\u001b[38;5;124m\"\u001b[39m, response)\n",
      "File \u001b[0;32m~/Programming Projects/Pearson Live Training Repos/Oreilly-LLM-Development-Stack/venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:191\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    190\u001b[0m     emit_warning()\n\u001b[0;32m--> 191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming Projects/Pearson Live Training Repos/Oreilly-LLM-Development-Stack/venv/lib/python3.10/site-packages/langchain/chains/base.py:603\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    602\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    604\u001b[0m         _output_key\n\u001b[1;32m    605\u001b[0m     ]\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[1;32m    609\u001b[0m         _output_key\n\u001b[1;32m    610\u001b[0m     ]\n",
      "File \u001b[0;32m~/Programming Projects/Pearson Live Training Repos/Oreilly-LLM-Development-Stack/venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:191\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    190\u001b[0m     emit_warning()\n\u001b[0;32m--> 191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming Projects/Pearson Live Training Repos/Oreilly-LLM-Development-Stack/venv/lib/python3.10/site-packages/langchain/chains/base.py:386\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \n\u001b[1;32m    356\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    379\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[1;32m    384\u001b[0m }\n\u001b[0;32m--> 386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming Projects/Pearson Live Training Repos/Oreilly-LLM-Development-Stack/venv/lib/python3.10/site-packages/langchain/chains/base.py:167\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    166\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 167\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    168\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m~/Programming Projects/Pearson Live Training Repos/Oreilly-LLM-Development-Stack/venv/lib/python3.10/site-packages/langchain/chains/base.py:157\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    156\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 157\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    160\u001b[0m     )\n\u001b[1;32m    162\u001b[0m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    163\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    164\u001b[0m     )\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Programming Projects/Pearson Live Training Repos/Oreilly-LLM-Development-Stack/venv/lib/python3.10/site-packages/langchain/agents/agent.py:1620\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m-> 1620\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1623\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1624\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1627\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m   1628\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[1;32m   1629\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[1;32m   1630\u001b[0m         )\n",
      "File \u001b[0;32m~/Programming Projects/Pearson Live Training Repos/Oreilly-LLM-Development-Stack/venv/lib/python3.10/site-packages/langchain/agents/agent.py:1326\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[1;32m   1318\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1319\u001b[0m     name_to_tool_map: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1323\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1324\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[0;32m-> 1326\u001b[0m         [\n\u001b[1;32m   1327\u001b[0m             a\n\u001b[1;32m   1328\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_next_step(\n\u001b[1;32m   1329\u001b[0m                 name_to_tool_map,\n\u001b[1;32m   1330\u001b[0m                 color_mapping,\n\u001b[1;32m   1331\u001b[0m                 inputs,\n\u001b[1;32m   1332\u001b[0m                 intermediate_steps,\n\u001b[1;32m   1333\u001b[0m                 run_manager,\n\u001b[1;32m   1334\u001b[0m             )\n\u001b[1;32m   1335\u001b[0m         ]\n\u001b[1;32m   1336\u001b[0m     )\n",
      "File \u001b[0;32m~/Programming Projects/Pearson Live Training Repos/Oreilly-LLM-Development-Stack/venv/lib/python3.10/site-packages/langchain/agents/agent.py:1326\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[1;32m   1318\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1319\u001b[0m     name_to_tool_map: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1323\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1324\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[0;32m-> 1326\u001b[0m         [\n\u001b[1;32m   1327\u001b[0m             a\n\u001b[1;32m   1328\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_next_step(\n\u001b[1;32m   1329\u001b[0m                 name_to_tool_map,\n\u001b[1;32m   1330\u001b[0m                 color_mapping,\n\u001b[1;32m   1331\u001b[0m                 inputs,\n\u001b[1;32m   1332\u001b[0m                 intermediate_steps,\n\u001b[1;32m   1333\u001b[0m                 run_manager,\n\u001b[1;32m   1334\u001b[0m             )\n\u001b[1;32m   1335\u001b[0m         ]\n\u001b[1;32m   1336\u001b[0m     )\n",
      "File \u001b[0;32m~/Programming Projects/Pearson Live Training Repos/Oreilly-LLM-Development-Stack/venv/lib/python3.10/site-packages/langchain/agents/agent.py:1365\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1363\u001b[0m     raise_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raise_error:\n\u001b[0;32m-> 1365\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1366\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn output parsing error occurred. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1367\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to pass this error back to the agent and have it try \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1368\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magain, pass `handle_parsing_errors=True` to the AgentExecutor. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1369\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is the error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1370\u001b[0m     )\n\u001b[1;32m   1371\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "\u001b[0;31mValueError\u001b[0m: An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: `Thought: It seems like I've gathered a lot of information about what causes the most CO2 emissions and how different sectors contribute to those emissions.\n\nAction: None needed this time!\n`\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE "
     ]
    }
   ],
   "source": [
    "from langchain.agents import Tool, initialize_agent\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.llms import Ollama\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# 1. Load and embed local document\n",
    "loader = TextLoader(\"climate.txt\")\n",
    "docs = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100).split_documents(loader.load())\n",
    "vs = Chroma.from_documents(docs, OllamaEmbeddings(model=\"mxbai-embed-large\"), persist_directory=\"lc_agent_chroma\")\n",
    "retriever = vs.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 4})\n",
    "\n",
    "# 2. Build QA chain on top of retriever\n",
    "llm = Ollama(model=\"llama3\")\n",
    "rag_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "\n",
    "# 3. Add Wikipedia as external tool\n",
    "wiki_tool = WikipediaAPIWrapper()\n",
    "\n",
    "# 4. Wrap both tools as LangChain Tool objects\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"LocalDocQA\",\n",
    "        func=rag_chain.run,\n",
    "        description=\"Use this tool to answer questions about climate change using the local document.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"WikipediaSearch\",\n",
    "        func=wiki_tool.run,\n",
    "        description=\"Use this when the local document does not seem to contain the information.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# 5. Initialize the agent\n",
    "agent_executor = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# 6. Run the agent\n",
    "response = agent_executor.run(\"What causes the most CO2 emissions?\")\n",
    "print(\"[Agentic LangChain]\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cb7274-2d2e-41b4-b596-d5c831f811b7",
   "metadata": {},
   "source": [
    "What happens in the background? Let's expose the call that the \"agent\" program makes, the responses of the LLM, and the loops that occur until the LLM surfaces the expected answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "311f07c4-1643-4f64-8578-4cd23ae79c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Prompt Template (ReAct Format with Placeholders) ===\n",
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "LocalDocQA(*args: Any, callbacks: Union[list[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = None, tags: Optional[list[str]] = None, metadata: Optional[dict[str, Any]] = None, **kwargs: Any) -> Any - Use this tool to answer questions about climate change using the local document.\n",
      "WikipediaSearch(query: str) -> str - Use this when the local document does not contain the needed information.\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [LocalDocQA, WikipediaSearch]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: {input}\n",
      "Thought:{agent_scratchpad}\n",
      "\n",
      "=== Rendered Prompt Sent to the LLM ===\n",
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "LocalDocQA(*args: Any, callbacks: Union[list[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = None, tags: Optional[list[str]] = None, metadata: Optional[dict[str, Any]] = None, **kwargs: Any) -> Any - Use this tool to answer questions about climate change using the local document.\n",
      "WikipediaSearch(query: str) -> str - Use this when the local document does not contain the needed information.\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [LocalDocQA, WikipediaSearch]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: What causes the most CO2 emissions?\n",
      "Thought:\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: The first thing that comes to mind is that I should check if the local document has any information on this topic. Maybe it has some statistics or data that can help me answer the question.\n",
      "\n",
      "Action: LocalDocQA\n",
      "Action Input: \"CO2 emissions\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe largest contributor to CO2 emissions is the burning of fossil fuels for energy and transportation. This includes coal-fired power plants, gasoline-powered vehicles, and industrial processes like cement production. Deforestation also contributes by reducing the planet's ability to absorb CO2.\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: `Thought: So, it looks like the local document has some information on what causes the most CO2 emissions. The observation suggests that burning fossil fuels is a significant contributor. I should think about whether this information alone is sufficient to answer the question.\n\nAction: None (since the local document already provided relevant information)\n`\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[0;32m~/Programming Projects/Pearson Live Training Repos/Oreilly-LLM-Development-Stack/venv/lib/python3.10/site-packages/langchain/agents/agent.py:1354\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[0;32m-> 1354\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_action_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1357\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Programming Projects/Pearson Live Training Repos/Oreilly-LLM-Development-Stack/venv/lib/python3.10/site-packages/langchain/agents/agent.py:801\u001b[0m, in \u001b[0;36mAgent.plan\u001b[0;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    800\u001b[0m full_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mpredict(callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfull_inputs)\n\u001b[0;32m--> 801\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming Projects/Pearson Live Training Repos/Oreilly-LLM-Development-Stack/venv/lib/python3.10/site-packages/langchain/agents/mrkl/output_parser.py:88\u001b[0m, in \u001b[0;36mMRKLOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]*Action\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*Input\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*:[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]*(.*)\u001b[39m\u001b[38;5;124m\"\u001b[39m, text, re\u001b[38;5;241m.\u001b[39mDOTALL\n\u001b[1;32m     87\u001b[0m ):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not parse LLM output: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     90\u001b[0m         observation\u001b[38;5;241m=\u001b[39mMISSING_ACTION_INPUT_AFTER_ACTION_ERROR_MESSAGE,\n\u001b[1;32m     91\u001b[0m         llm_output\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m     92\u001b[0m         send_to_llm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     93\u001b[0m     )\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Could not parse LLM output: `Thought: So, it looks like the local document has some information on what causes the most CO2 emissions. The observation suggests that burning fossil fuels is a significant contributor. I should think about whether this information alone is sufficient to answer the question.\n\nAction: None (since the local document already provided relevant information)\n`\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 62\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(formatted_prompt\u001b[38;5;241m.\u001b[39mto_string())\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Run the agent\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat causes the most CO2 emissions?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Agentic LangChain]\u001b[39m\u001b[38;5;124m\"\u001b[39m, response)\n",
      "File \u001b[0;32m~/Programming Projects/Pearson Live Training Repos/Oreilly-LLM-Development-Stack/venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:191\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    190\u001b[0m     emit_warning()\n\u001b[0;32m--> 191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming Projects/Pearson Live Training Repos/Oreilly-LLM-Development-Stack/venv/lib/python3.10/site-packages/langchain/chains/base.py:603\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    602\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    604\u001b[0m         _output_key\n\u001b[1;32m    605\u001b[0m     ]\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[1;32m    609\u001b[0m         _output_key\n\u001b[1;32m    610\u001b[0m     ]\n",
      "File \u001b[0;32m~/Programming Projects/Pearson Live Training Repos/Oreilly-LLM-Development-Stack/venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:191\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    190\u001b[0m     emit_warning()\n\u001b[0;32m--> 191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming Projects/Pearson Live Training Repos/Oreilly-LLM-Development-Stack/venv/lib/python3.10/site-packages/langchain/chains/base.py:386\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \n\u001b[1;32m    356\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    379\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[1;32m    384\u001b[0m }\n\u001b[0;32m--> 386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming Projects/Pearson Live Training Repos/Oreilly-LLM-Development-Stack/venv/lib/python3.10/site-packages/langchain/chains/base.py:167\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    166\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 167\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    168\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m~/Programming Projects/Pearson Live Training Repos/Oreilly-LLM-Development-Stack/venv/lib/python3.10/site-packages/langchain/chains/base.py:157\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    156\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 157\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    160\u001b[0m     )\n\u001b[1;32m    162\u001b[0m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    163\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    164\u001b[0m     )\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Programming Projects/Pearson Live Training Repos/Oreilly-LLM-Development-Stack/venv/lib/python3.10/site-packages/langchain/agents/agent.py:1620\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m-> 1620\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1623\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1624\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1627\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m   1628\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[1;32m   1629\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[1;32m   1630\u001b[0m         )\n",
      "File \u001b[0;32m~/Programming Projects/Pearson Live Training Repos/Oreilly-LLM-Development-Stack/venv/lib/python3.10/site-packages/langchain/agents/agent.py:1326\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[1;32m   1318\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1319\u001b[0m     name_to_tool_map: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1323\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1324\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[0;32m-> 1326\u001b[0m         [\n\u001b[1;32m   1327\u001b[0m             a\n\u001b[1;32m   1328\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_next_step(\n\u001b[1;32m   1329\u001b[0m                 name_to_tool_map,\n\u001b[1;32m   1330\u001b[0m                 color_mapping,\n\u001b[1;32m   1331\u001b[0m                 inputs,\n\u001b[1;32m   1332\u001b[0m                 intermediate_steps,\n\u001b[1;32m   1333\u001b[0m                 run_manager,\n\u001b[1;32m   1334\u001b[0m             )\n\u001b[1;32m   1335\u001b[0m         ]\n\u001b[1;32m   1336\u001b[0m     )\n",
      "File \u001b[0;32m~/Programming Projects/Pearson Live Training Repos/Oreilly-LLM-Development-Stack/venv/lib/python3.10/site-packages/langchain/agents/agent.py:1326\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[1;32m   1318\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1319\u001b[0m     name_to_tool_map: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1323\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1324\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[0;32m-> 1326\u001b[0m         [\n\u001b[1;32m   1327\u001b[0m             a\n\u001b[1;32m   1328\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_next_step(\n\u001b[1;32m   1329\u001b[0m                 name_to_tool_map,\n\u001b[1;32m   1330\u001b[0m                 color_mapping,\n\u001b[1;32m   1331\u001b[0m                 inputs,\n\u001b[1;32m   1332\u001b[0m                 intermediate_steps,\n\u001b[1;32m   1333\u001b[0m                 run_manager,\n\u001b[1;32m   1334\u001b[0m             )\n\u001b[1;32m   1335\u001b[0m         ]\n\u001b[1;32m   1336\u001b[0m     )\n",
      "File \u001b[0;32m~/Programming Projects/Pearson Live Training Repos/Oreilly-LLM-Development-Stack/venv/lib/python3.10/site-packages/langchain/agents/agent.py:1365\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1363\u001b[0m     raise_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raise_error:\n\u001b[0;32m-> 1365\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1366\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn output parsing error occurred. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1367\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to pass this error back to the agent and have it try \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1368\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magain, pass `handle_parsing_errors=True` to the AgentExecutor. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1369\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is the error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1370\u001b[0m     )\n\u001b[1;32m   1371\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "\u001b[0;31mValueError\u001b[0m: An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: `Thought: So, it looks like the local document has some information on what causes the most CO2 emissions. The observation suggests that burning fossil fuels is a significant contributor. I should think about whether this information alone is sufficient to answer the question.\n\nAction: None (since the local document already provided relevant information)\n`\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE "
     ]
    }
   ],
   "source": [
    "from langchain.agents import Tool, initialize_agent\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.llms import Ollama\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# Load and prepare documents\n",
    "loader = TextLoader(\"climate.txt\")\n",
    "docs = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100).split_documents(loader.load())\n",
    "vs = Chroma.from_documents(docs, OllamaEmbeddings(model=\"mxbai-embed-large\"), persist_directory=\"lc_agent_chroma\")\n",
    "retriever = vs.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 4})\n",
    "llm = Ollama(model=\"llama3\")\n",
    "\n",
    "# Define tools\n",
    "rag_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "wiki_tool = WikipediaAPIWrapper()\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"LocalDocQA\",\n",
    "        func=rag_chain.run,\n",
    "        description=\"Use this tool to answer questions about climate change using the local document.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"WikipediaSearch\",\n",
    "        func=wiki_tool.run,\n",
    "        description=\"Use this when the local document does not contain the needed information.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Initialize the agent\n",
    "agent_executor = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Insert this to reveal the internal LLM prompt\n",
    "from langchain.agents import AgentOutputParser\n",
    "\n",
    "# Show the base ReAct-style prompt template\n",
    "print(\"=== Prompt Template (ReAct Format with Placeholders) ===\")\n",
    "print(agent_executor.agent.llm_chain.prompt.template)\n",
    "\n",
    "# Show the full rendered prompt as it would be sent to the LLM\n",
    "formatted_prompt = agent_executor.agent.llm_chain.prompt.format_prompt(\n",
    "    input=\"What causes the most CO2 emissions?\",\n",
    "    tools=tools,\n",
    "    tool_names=[tool.name for tool in tools],\n",
    "    agent_scratchpad=\"\"  # This avoids the KeyError\n",
    ")\n",
    "print(\"\\n=== Rendered Prompt Sent to the LLM ===\")\n",
    "print(formatted_prompt.to_string())\n",
    "\n",
    "\n",
    "# Run the agent\n",
    "response = agent_executor.run(\"What causes the most CO2 emissions?\")\n",
    "print(\"[Agentic LangChain]\", response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d26694-461f-4fa5-b4a6-46cf7eacc1aa",
   "metadata": {},
   "source": [
    "## Building a Custom Agent with LangGraph (Step-by-Step)\n",
    "\n",
    "This section of the notebook demonstrates how to build a **simple, explainable agent** using **LangGraph**, mimicking the behavior of a LangChain `ZERO_SHOT_REACT_DESCRIPTION` agent — but with full control over its logic, flow, and execution.\n",
    "\n",
    "The effect is the same as the previous example, so you are on familiar ground, but the interest is that LangGraph allows you to build your own agent and its structure, so you can go beyond pre-canned tools.\n",
    "---\n",
    "\n",
    "##  Step 1: Set Up Tools\n",
    "\n",
    "This step loads and prepares everything needed to power the agent's capabilities.\n",
    "\n",
    "**Functions and Components Used:**\n",
    "- `TextLoader`: Loads the `climate.txt` document from disk.\n",
    "- `RecursiveCharacterTextSplitter`: Splits the document into overlapping chunks for better semantic search.\n",
    "- `OllamaEmbeddings`: Creates vector embeddings using a local model (e.g., `mxbai-embed-large`).\n",
    "- `Chroma`: Stores those embeddings for efficient retrieval.\n",
    "- `Ollama`: Runs the local language model (`llama3`) used for answering.\n",
    "- `RetrievalQA`: A wrapper that connects the retriever and the LLM.\n",
    "- `WikipediaAPIWrapper`: A tool that performs a Wikipedia search as a fallback.\n",
    "\n",
    " **Purpose:** These tools form the basis for all reasoning and information retrieval used later by the agent.\n",
    "\n",
    "\n",
    "##  Step 2: Define the Agent's State\n",
    "\n",
    "LangGraph requires a formal declaration of what data flows through your graph. This step defines that shared memory.\n",
    "\n",
    "**Structure: `QAState`**\n",
    "- `question`: The user’s query.\n",
    "- `rag_answer`: Stores the answer if the RAG tool was used.\n",
    "- `wiki_answer`: Stores the answer if the Wikipedia tool was used.\n",
    "- `answer`: The final answer to return. This key is marked with `Annotated[..., \"output\"]` so LangGraph knows to treat it as a **single-write endpoint**.\n",
    "\n",
    " **Purpose:** Controls the input, intermediate, and output values handled by each node in the graph.\n",
    "\n",
    "\n",
    "##  Step 3: Routing Logic\n",
    "\n",
    "This is the \"agentic brain\" — a custom Python function that decides *which tool to use* based on the question.\n",
    "\n",
    "**Function: `tool_selector(state)`**\n",
    "- Inspects `state[\"question\"]`\n",
    "- If the question contains the word `\"climate\"`, it returns `{\"__next__\": \"rag\"}`\n",
    "- Otherwise, it returns `{\"__next__\": \"wiki\"}`\n",
    "\n",
    " **Purpose:** Simulates an intelligent decision-maker — your own custom routing logic instead of relying on a hardcoded or LLM-parsed policy.\n",
    "\n",
    "\n",
    "##  Step 4: Define Graph Nodes\n",
    "\n",
    "Each node is a callable function that:\n",
    "- Accepts the current state\n",
    "- Returns a dictionary with updated values\n",
    "\n",
    "**Node Functions:**\n",
    "- `rag_node(state)`: Calls the RAG tool and stores the output in `rag_answer`.\n",
    "- `wiki_node(state)`: Calls Wikipedia and stores the output in `wiki_answer`.\n",
    "- `merger(state)`: Consolidates the answers. If `rag_answer` is present, it uses it. Otherwise, it falls back to `wiki_answer`. The result is stored in `answer`.\n",
    "\n",
    " **Purpose:** These nodes act like \"agent limbs\" — executing concrete actions depending on what the brain (`tool_selector`) decided.\n",
    "\n",
    "\n",
    "##  Step 5: Build the Graph\n",
    "\n",
    "Here we assemble the agent as a directed graph of reasoning steps.\n",
    "\n",
    "**Steps:**\n",
    "- Instantiate the graph: `graph = StateGraph(QAState)`\n",
    "- Add nodes: router, rag, wiki, merger\n",
    "- Connect edges:\n",
    "  - `router → rag`\n",
    "  - `router → wiki`\n",
    "  - `rag → merger`\n",
    "  - `wiki → merger`\n",
    "  - `merger → END`\n",
    "- Compile the graph: `agent = graph.compile()`\n",
    "\n",
    " **Purpose:** This defines the possible paths an input can take — making the reasoning traceable and testable.\n",
    "\n",
    "\n",
    "##  Step 6: Run the Agent\n",
    "\n",
    "We finally invoke the agent with a natural language question.\n",
    "\n",
    "**Invocation:**\n",
    "```python\n",
    "question = \"What causes the most CO2 emissions?\"\n",
    "result = agent.invoke({\"question\": question})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa4d202c-a236-4420-ac0e-7ed88490122e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LangGraph Agentic] According to the context, the largest contributor to CO2 emissions is \"the burning of fossil fuels for energy and transportation.\"\n"
     ]
    }
   ],
   "source": [
    "# ---------------- LangGraph Agentic Example ----------------\n",
    "\n",
    "from typing import TypedDict, Annotated, Optional, Union\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# === Step 1: Set up tools ===\n",
    "docs = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100).split_documents(\n",
    "    TextLoader(\"climate.txt\").load()\n",
    ")\n",
    "vs = Chroma.from_documents(docs, OllamaEmbeddings(model=\"mxbai-embed-large\"), persist_directory=\"graph_chroma\")\n",
    "retriever = vs.as_retriever()\n",
    "llm = Ollama(model=\"llama3\")\n",
    "rag_tool = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "wiki_tool = WikipediaAPIWrapper()\n",
    "\n",
    "# === Step 2: Define state ===\n",
    "class QAState(TypedDict):\n",
    "    question: str\n",
    "    rag_answer: Optional[str]\n",
    "    wiki_answer: Optional[str]\n",
    "    answer: Annotated[Optional[str], \"output\"]  # Final output goes here\n",
    "\n",
    "# === Step 3: Routing logic ===\n",
    "def tool_selector(state: QAState):\n",
    "    if \"climate\" in state[\"question\"].lower():\n",
    "        return {\"__next__\": \"rag\"}\n",
    "    else:\n",
    "        return {\"__next__\": \"wiki\"}\n",
    "\n",
    "# === Step 4: Nodes ===\n",
    "def rag_node(state: QAState):\n",
    "    return {\"rag_answer\": rag_tool.run(state[\"question\"])}\n",
    "\n",
    "def wiki_node(state: QAState):\n",
    "    return {\"wiki_answer\": wiki_tool.run(state[\"question\"])}\n",
    "\n",
    "def merger(state: QAState):\n",
    "    return {\"answer\": state.get(\"rag_answer\") or state.get(\"wiki_answer\")}\n",
    "\n",
    "# === Step 5: Build graph ===\n",
    "graph = StateGraph(QAState)\n",
    "graph.add_node(\"router\", RunnableLambda(tool_selector))\n",
    "graph.add_node(\"rag\", rag_node)\n",
    "graph.add_node(\"wiki\", wiki_node)\n",
    "graph.add_node(\"merger\", merger)\n",
    "\n",
    "graph.set_entry_point(\"router\")\n",
    "graph.add_edge(\"router\", \"rag\")\n",
    "graph.add_edge(\"router\", \"wiki\")\n",
    "graph.add_edge(\"rag\", \"merger\")\n",
    "graph.add_edge(\"wiki\", \"merger\")\n",
    "graph.add_edge(\"merger\", END)\n",
    "\n",
    "agent = graph.compile()\n",
    "\n",
    "# === Step 6: Run it ===\n",
    "question = \"What causes the most CO2 emissions?\"\n",
    "result = agent.invoke({\"question\": question})\n",
    "print(\"[LangGraph Agentic]\", result[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8233b589-676c-4975-b219-4bb50d226a6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM Stack)",
   "language": "python",
   "name": "llm-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
