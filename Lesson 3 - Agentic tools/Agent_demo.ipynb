{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4154ad73-07b8-4b84-82d5-3c1e499bdcec",
   "metadata": {},
   "source": [
    "# Manual Agentic AI – Pure Python Baseline\n",
    "\n",
    "This notebook demonstrates a basic example of agentic behavior first using pure Python, without relying on high-level orchestration frameworks like LangChain or CrewAI.\n",
    "\n",
    "We define an agent-like structure by combining:\n",
    "- **Perception:** Load and embed document content into vector representations.\n",
    "- **Planning/Reasoning:** Retrieve the most relevant paragraph for a question using semantic similarity.\n",
    "- **Action:** Use a question-answering model to extract an answer from the selected context.\n",
    "\n",
    "This hands-on example shows how we can *manually construct agentic behavior* using existing NLP tools such as `sentence-transformers` and `transformers`.\n",
    "\n",
    "In later parts of the notebook, we will revisit this same logic using LangChain and CrewAI to contrast manual and framework-based approaches to agentic AI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7961fb2-dd2f-46b5-baf2-86a9aeb489fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pure Python] burning of fossil fuels\n"
     ]
    }
   ],
   "source": [
    "# Agentic LLMs Demo with Local Ollama\n",
    "# Goal: Student uploads a climate document. Agents can: summarize, answer question, use tools (web search if needed).\n",
    "# Implemented progressively with: Pure Python, LangChain, LlamaIndex, CrewAI, AutoGen\n",
    "\n",
    "# ------------------- Pure Python Agentic AI -------------------\n",
    "\n",
    "# Import required libraries for sentence embedding and question answering\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the sentence transformer model for converting text to embeddings.\n",
    "# This will allow us to compare text paragraphs and questions semantically.\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load a question-answering pipeline using a pretrained transformer model.\n",
    "# This model will extract answers from the relevant paragraph we provide.\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
    "\n",
    "# ------------------- Perception Step: Ingest and Encode Knowledge -------------------\n",
    "\n",
    "# Load the knowledge base from a local text file.\n",
    "# Assume each paragraph is separated by two line breaks (\"\\n\\n\").\n",
    "with open(\"climate.txt\") as f:\n",
    "    paragraphs = f.read().split(\"\\n\\n\")\n",
    "\n",
    "# Convert each paragraph into a vector representation (embedding).\n",
    "# These embeddings will later be compared against the question to find the best match.\n",
    "embeddings = embedder.encode(paragraphs, convert_to_tensor=True)\n",
    "\n",
    "# ------------------- Planning Step: Interpret Question and Select Context -------------------\n",
    "\n",
    "# Define a user question – this is the task the agent will try to solve.\n",
    "question = \"What causes the most CO2 emissions?\"\n",
    "\n",
    "# Convert the question into the same embedding space as the document.\n",
    "q_embedding = embedder.encode(question, convert_to_tensor=True)\n",
    "\n",
    "# Compute cosine similarity between the question and each paragraph.\n",
    "# Find the paragraph most semantically similar to the question.\n",
    "idx = util.pytorch_cos_sim(q_embedding, embeddings)[0].argmax()\n",
    "\n",
    "# Retrieve the best-matching paragraph as the context for answering the question.\n",
    "context = paragraphs[idx]\n",
    "\n",
    "# ------------------- Action Step: Answer the Question -------------------\n",
    "\n",
    "# Pass the question and selected context to the QA model.\n",
    "# The model returns a short answer extracted from the context.\n",
    "result = qa_pipeline(question=question, context=context)\n",
    "\n",
    "# Display the answer to the user.\n",
    "print(\"[Pure Python]\", result[\"answer\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5404e9e8-e911-4afe-9279-78dc142ff239",
   "metadata": {},
   "source": [
    "# Agent with Decision-Making Logic (Multi-Source QA)\n",
    "\n",
    "In this example, we extend the previous agent to include a simple decision-making mechanism.\n",
    "\n",
    "The agent tries to answer the question using a local document (`climate.txt`). If the most relevant paragraph from that document is not semantically close enough to the question, the agent assumes that the document does not contain the needed information.\n",
    "\n",
    "It then **falls back to a secondary source** — here, Wikipedia — and attempts to retrieve a summary related to the question and answer based on that.\n",
    "\n",
    "This illustrates:\n",
    "- Basic **planning** (decide between multiple tools/sources),\n",
    "- Use of **semantic similarity threshold** to evaluate confidence,\n",
    "- Use of external resources for dynamic behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e500c9a-3679-41db-a0a4-8a6feb8fbc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using local context (score=0.80)\n",
      "[Agent Answer] burning of fossil fuels\n"
     ]
    }
   ],
   "source": [
    "# ------------------- Agentic QA with Fallback -------------------\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import pipeline\n",
    "import wikipedia\n",
    "\n",
    "# Load models\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
    "\n",
    "# Load document and compute embeddings - try climate.txt and Noclimate.txt\n",
    "with open(\"climate.txt\") as f:\n",
    "    paragraphs = f.read().split(\"\\n\\n\")\n",
    "embeddings = embedder.encode(paragraphs, convert_to_tensor=True)\n",
    "\n",
    "# Define the question\n",
    "question = \"What causes the most CO2 emissions?\"\n",
    "q_embedding = embedder.encode(question, convert_to_tensor=True)\n",
    "\n",
    "# Compute similarity between the question and all paragraphs\n",
    "cosine_similarities = util.pytorch_cos_sim(q_embedding, embeddings)[0]\n",
    "best_idx = cosine_similarities.argmax()\n",
    "best_score = cosine_similarities[best_idx].item()\n",
    "best_context = paragraphs[best_idx]\n",
    "\n",
    "# Define a confidence threshold (tunable)\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "if best_score > THRESHOLD:\n",
    "    print(f\"Using local context (score={best_score:.2f})\")\n",
    "    context = best_context\n",
    "else:\n",
    "    print(f\"Local context not relevant enough (score={best_score:.2f}), using Wikipedia instead.\")\n",
    "    \n",
    "    # Fallback: Use Wikipedia summary as external context\n",
    "    wiki = Wikipedia(\n",
    "    language=\"en\",\n",
    "    user_agent=\"AgenticAITutorial/1.0 (yourname@example.com)\"\n",
    "    )\n",
    "    wikipedia.set_rate_limiting(True)\n",
    "\n",
    "    try:\n",
    "        # Search Wikipedia for the most relevant page titles\n",
    "        search_results = wikipedia.search(question)\n",
    "    \n",
    "        if search_results:\n",
    "            # Use the top result\n",
    "            page_title = search_results[0]\n",
    "            context = wikipedia.summary(page_title, sentences=5)\n",
    "            print(f\"Using Wikipedia page: {page_title}\")\n",
    "        else:\n",
    "            context = \"No relevant Wikipedia search result found.\"\n",
    "            print(context)\n",
    "\n",
    "    except Exception as e:\n",
    "        context = f\"Error retrieving from Wikipedia: {str(e)}\"\n",
    "        print(context)\n",
    "\n",
    "# Run question-answering on selected context\n",
    "result = qa_pipeline(question=question, context=context)\n",
    "print(\"[Agent Answer]\", result[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad996d7-21bb-4ec7-b4d8-1a62d0358435",
   "metadata": {},
   "source": [
    "# Agentic QA with LangChain and Ollama\n",
    "\n",
    "In this version, we implement the same logic as the manual agentic example — retrieve context, answer a question — but using **LangChain**, a framework that abstracts and automates agentic patterns.\n",
    "\n",
    "LangChain allows us to:\n",
    "- Load and split documents into chunks,\n",
    "- Convert these chunks into vector embeddings and store them in a vector database,\n",
    "- Retrieve the most relevant chunk using semantic search (e.g. via Max Marginal Relevance),\n",
    "- Send the selected context and question to a language model for answering.\n",
    "\n",
    "We use the following LangChain modules:\n",
    "- `TextLoader` to load the document,\n",
    "- `RecursiveCharacterTextSplitter` to chunk it,\n",
    "- `OllamaEmbeddings` to generate vector embeddings (via a local embedding model),\n",
    "- `Chroma` as the vector database,\n",
    "- `RetrievalQA` to wrap everything into an end-to-end QA system.\n",
    "\n",
    "This implementation demonstrates how **agentic workflows** (perception → retrieval → reasoning → output) can be modularized and scaled using LangChain components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c54ec69-a33b-4179-ab18-57fdf4cdffea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f4/sswmwmgd3bl8rxfdhchzt6h00000gn/T/ipykernel_19319/764880918.py:14: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
      "/var/folders/f4/sswmwmgd3bl8rxfdhchzt6h00000gn/T/ipykernel_19319/764880918.py:18: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"llama3\")\n",
      "/var/folders/f4/sswmwmgd3bl8rxfdhchzt6h00000gn/T/ipykernel_19319/764880918.py:20: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(\"[LangChain]\", qa.run(\"What causes the most CO2 emissions?\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LangChain] The largest contributor to CO2 emissions is the burning of fossil fuels for energy and transportation. This includes coal-fired power plants, gasoline-powered vehicles, and industrial processes like cement production.\n",
      "[LangChain] {'query': 'What causes the most CO2 emissions?', 'result': 'The largest contributor to CO2 emissions is the burning of fossil fuels for energy and transportation. This includes coal-fired power plants, gasoline-powered vehicles, and industrial processes like cement production.'}\n"
     ]
    }
   ],
   "source": [
    "# --------------- LANGCHAIN + OLLAMA ---------------\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "loader = TextLoader(\"climate.txt\")\n",
    "raw_docs = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "docs = splitter.split_documents(raw_docs)\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "vs = Chroma.from_documents(docs, embeddings, persist_directory=\"lc_chroma\")\n",
    "retriever = vs.as_retriever(search_type=\"mmr\")\n",
    "\n",
    "llm = Ollama(model=\"llama3\")\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "print(\"[LangChain]\", qa.run(\"What causes the most CO2 emissions?\"))\n",
    "# --------------- LANGCHAIN + OLLAMA -------------------\n",
    "\n",
    "# Load the required LangChain components\n",
    "from langchain_community.llms import Ollama                    # Local LLM interface (e.g., llama3)\n",
    "from langchain.embeddings import OllamaEmbeddings              # Embedding model via Ollama (e.g., mxbai-embed-large)\n",
    "from langchain.vectorstores import Chroma                      # Vector store to store and search chunk embeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Splits long text into manageable chunks\n",
    "from langchain.document_loaders import TextLoader              # Utility to load text files as documents\n",
    "from langchain.chains import RetrievalQA                       # Retrieval-augmented QA pipeline\n",
    "\n",
    "# ------------------- Perception: Load and Prepare Knowledge -------------------\n",
    "\n",
    "# Load the raw document from file\n",
    "loader = TextLoader(\"climate.txt\")\n",
    "raw_docs = loader.load()  # List of Document objects\n",
    "\n",
    "# Split the document into smaller overlapping chunks for better semantic matching\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "docs = splitter.split_documents(raw_docs)\n",
    "\n",
    "# ------------------- Representation: Embed and Store Chunks -------------------\n",
    "\n",
    "# Load a local embedding model via Ollama (can be changed to any compatible embedding model)\n",
    "embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "\n",
    "# Create or load a Chroma vector store from the split chunks\n",
    "vs = Chroma.from_documents(docs, embeddings, persist_directory=\"lc_chroma\")\n",
    "\n",
    "# Create a retriever using Max Marginal Relevance (MMR) to reduce redundancy\n",
    "retriever = vs.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 4})\n",
    "\n",
    "# ------------------- Reasoning: Setup the LLM -------------------\n",
    "\n",
    "# Load the local language model (e.g., LLaMA 3 via Ollama)\n",
    "llm = Ollama(model=\"llama3\")\n",
    "\n",
    "# Wrap everything in a RetrievalQA chain: retrieve relevant chunk, pass to LLM, and get answer\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "\n",
    "# ------------------- Action: Ask a Question -------------------\n",
    "\n",
    "# Run the full agentic pipeline\n",
    "print(\"[LangChain]\", qa.invoke(\"What causes the most CO2 emissions?\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f66edd-d724-46c0-b39c-38481473bbb0",
   "metadata": {},
   "source": [
    "This LangChain RAG setup behaves like an agent in a sandbox: it perceives its environment (a fixed document), chooses the most relevant information (via retrieval), i.e. it makes decisions, and takes a single goal-driven action (answering the question). It’s not a fully autonomous agent, because it does not select between multiple possible tools, it does not need to keep memory of past steps, does not need multi-step reasoning — but it captures the core structure of one. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96e693d3-3cc8-43e2-a461-37f66f6fbfc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LlamaIndex] The burning of fossil fuels for energy and transportation.\n"
     ]
    }
   ],
   "source": [
    "# --------------- LLAMAINDEX + OLLAMA ---------------\n",
    "\n",
    "# Import the vector index and document loader\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "# Import Ollama-compatible embedding and LLM modules\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.llms.ollama import Ollama as LlamaIndexOllama\n",
    "\n",
    "# ------------------- Perception: Load and Understand Data -------------------\n",
    "\n",
    "# Instantiate an Ollama-powered LLM (e.g., llama3)\n",
    "llm = LlamaIndexOllama(model=\"llama3\")\n",
    "\n",
    "# Create an embedding model for representing text chunks semantically\n",
    "embed = OllamaEmbedding(model_name=\"mxbai-embed-large\")\n",
    "\n",
    "# Load the raw document(s) using LlamaIndex's simple file reader\n",
    "docs = SimpleDirectoryReader(input_files=[\"climate.txt\"]).load_data()\n",
    "\n",
    "# ------------------- Representation: Build Index -------------------\n",
    "\n",
    "# Create a vector index from the loaded documents using the embedding model\n",
    "index = VectorStoreIndex.from_documents(docs, embed_model=embed)\n",
    "\n",
    "# ------------------- Reasoning & Action: Query Engine -------------------\n",
    "\n",
    "# Create a query engine that:\n",
    "# - Uses the LLM to reason over chunks\n",
    "# - Automatically retrieves the most relevant content from the index\n",
    "qe = index.as_query_engine(llm=llm)\n",
    "\n",
    "# Ask a question and get the response from the LLM\n",
    "response = qe.query(\"What causes the most CO2 emissions?\")\n",
    "\n",
    "# Output the result\n",
    "print(\"[LlamaIndex]\", response)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b657ce11-0011-4293-ab88-5298f08f96d0",
   "metadata": {},
   "source": [
    "### Why This Is Agentic\n",
    "\n",
    "Even though the code looks shorter than LangChain’s version, the behavior is still *agentic* because:\n",
    "\n",
    "- **Autonomous Retrieval**: The system decides *what context to retrieve* from the index in response to a user query.\n",
    "- **LLM Reasoning**: The model interprets the question, synthesizes the answer, and doesn’t follow a rigid path.\n",
    "- **Separation of Concern**: The document is separated from the logic. The system autonomously chooses relevant chunks at runtime.\n",
    "- **Tool Use (Implied)**: The LLM is *using* the retrieval tool (index) to guide its generation — a core sign of agentic reasoning.\n",
    "\n",
    "---\n",
    "\n",
    "### Comparison with LangChain (in short)\n",
    "\n",
    "| Aspect              | LangChain                                                                 | LlamaIndex                                                   |\n",
    "|---------------------|---------------------------------------------------------------------------|---------------------------------------------------------------|\n",
    "| **Design Style**    | **Modular**: You explicitly wire LLM, embeddings, and tools               | **Monolithic**: Index handles loading, embedding, retrieval   |\n",
    "| **Control Granularity** | **Fine-grained** control over each step (useful for chaining tools)    | **High-level abstraction**, more compact but less modular     |\n",
    "| **Agentic Flow**    | **Explicit**: Each step can be reasoned about individually                | **Implicit**: Agentic behavior is packaged in `as_query_engine` |\n",
    "| **Ease of Use**     | Requires more setup but more flexible                                     | Easier to get started, excellent for document QA              |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "503b5ffd-e5d0-45d2-8295-792270166cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using OpenAI API key starts with: sk-proj-\n",
      "[INFO] Document length: 1292 characters\n",
      "[INFO] Starting crew execution...\n",
      "\n",
      "[FINAL OUTPUT]\n",
      "ISSUES:\n",
      "- Greenhouse gas emissions from human activities, particularly from fossil fuel burning and deforestation\n",
      "- Global warming leading to rising sea levels and extreme weather events\n",
      "- Ecosystem disruptions due to climate change impacts\n",
      "\n",
      "SOLUTIONS:\n",
      "- Transitioning to renewable energy sources to reduce reliance on fossil fuels\n",
      "- Implementing policies like carbon pricing to incentivize emission reductions\n",
      "- Enhancing global cooperation and commitment to climate action, as demonstrated in the Paris Agreement\n",
      "- Investing in adaptation measures such as building defenses and developing resilient crops to address the impacts of climate change\n"
     ]
    }
   ],
   "source": [
    "# --------------- CREWAI + OpenAI  ---------------\n",
    "# CrewAI does not support Ollama or local modesl by default, you can use LangChain if you need a local model, but with native CrewAI, use a cloud solution like OpenAI\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from crewai import Agent, Task, Crew\n",
    "\n",
    "# Step 1: Load API key\n",
    "load_dotenv(\"keys.env\")\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(\"[INFO] Using OpenAI API key starts with:\", api_key[:8])\n",
    "\n",
    "# Step 2: Define the LLM with stricter parameters\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=api_key, \n",
    "    model=\"gpt-3.5-turbo\", \n",
    "    temperature=0.1,  # Lower temperature for more focused responses\n",
    "    max_tokens=500    # Limit response length\n",
    ")\n",
    "\n",
    "# Step 3: Define Agents with more specific configurations\n",
    "summarizer = Agent(\n",
    "    role=\"Document Summarizer\",\n",
    "    goal=\"Read the provided climate document and create exactly one concise summary.\",\n",
    "    backstory=\"You are an expert at reading documents and creating brief summaries. You always complete your task in one attempt.\",\n",
    "    llm=llm,\n",
    "    verbose=False,  # Reduce verbosity to prevent loops\n",
    "    allow_delegation=False,\n",
    "    max_iter=1,  # Force completion in one iteration\n",
    "    memory=False  # Disable memory to prevent confusion\n",
    ")\n",
    "\n",
    "responder = Agent(\n",
    "    role=\"Climate QA Analyst\", \n",
    "    goal=\"Analyze the summary and extract key climate issues and solutions.\",\n",
    "    backstory=\"You are a climate policy expert who identifies key issues and solutions from summaries.\",\n",
    "    llm=llm,\n",
    "    verbose=False,\n",
    "    allow_delegation=False,\n",
    "    max_iter=1,\n",
    "    memory=False\n",
    ")\n",
    "\n",
    "# Step 4: Load document\n",
    "with open(\"climate.txt\", \"r\") as f:\n",
    "    doc = f.read()\n",
    "\n",
    "print(f\"[INFO] Document length: {len(doc)} characters\")\n",
    "\n",
    "# Step 5: Create Tasks with very specific, clear instructions\n",
    "t1 = Task(\n",
    "    description=f\"\"\"\n",
    "DOCUMENT TO SUMMARIZE:\n",
    "{doc}\n",
    "\n",
    "TASK: Write a summary of this document in 100-150 words. Focus on the main climate topics discussed.\n",
    "Do not repeat this instruction. Just provide the summary and nothing else.\n",
    "\"\"\",\n",
    "    expected_output=\"A 100-150 word summary of the climate document\",\n",
    "    agent=summarizer\n",
    ")\n",
    "\n",
    "t2 = Task(\n",
    "    description=\"\"\"\n",
    "Based on the summary from the previous task, identify:\n",
    "1. The 3 most important climate issues mentioned\n",
    "2. Any solutions or recommendations discussed\n",
    "\n",
    "Format your response as:\n",
    "ISSUES:\n",
    "- Issue 1\n",
    "- Issue 2  \n",
    "- Issue 3\n",
    "\n",
    "SOLUTIONS:\n",
    "- Solution 1\n",
    "- Solution 2\n",
    "- etc.\n",
    "\"\"\",\n",
    "    expected_output=\"A formatted list of climate issues and solutions\",\n",
    "    agent=responder,\n",
    "    context=[t1]\n",
    ")\n",
    "\n",
    "# Step 6: Create Crew with minimal configuration\n",
    "crew = Crew(\n",
    "    agents=[summarizer, responder],\n",
    "    tasks=[t1, t2],\n",
    "    verbose=False,  # Turn off verbose to reduce output\n",
    "    process=\"sequential\",\n",
    "    max_rpm=10,  # Limit requests per minute\n",
    "    memory=False  # Disable crew memory\n",
    ")\n",
    "\n",
    "try:\n",
    "    print(\"[INFO] Starting crew execution...\")\n",
    "    result = crew.kickoff()\n",
    "    print(\"\\n[FINAL OUTPUT]\")\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10ccd99-a4e6-4959-88d4-efe49af37d57",
   "metadata": {},
   "source": [
    "### Why This Is Agentic (CrewAI)\n",
    "\n",
    "This code demonstrates an **agentic system** using the **CrewAI framework**, where **multiple specialized agents** collaborate to solve a problem using an LLM. Here's why this qualifies as *agentic*:\n",
    "\n",
    "- **Role-Based Agents**: Each agent is defined with a clear *goal*, *backstory*, and *task-specific instructions* — emulating autonomous reasoning entities.\n",
    "- **Tool Use (LLM as Cognitive Engine)**: Each agent uses the LLM (OpenAI GPT-3.5-Turbo) to analyze, summarize, and reason through their part of the problem.\n",
    "- **Task-Driven Execution**: Agents are assigned distinct tasks with defined outputs, and the system coordinates the handoff (e.g., Task 2 builds on Task 1).\n",
    "- **No Fixed Script**: Although the process is `sequential`, the content and structure of the response are *dynamically generated* by the agents at runtime.\n",
    "- **Independent Execution Flow**: Agents operate independently, without loops or memory, using their configured reasoning capability.\n",
    "\n",
    "---\n",
    "\n",
    "### Why OpenAI Instead of Ollama?\n",
    "\n",
    "CrewAI **does not natively support local LLMs like Ollama**. Instead, it relies on:\n",
    "\n",
    "- The `langchain_openai.ChatOpenAI` wrapper to integrate OpenAI models (e.g., `gpt-3.5-turbo`, `gpt-4`)\n",
    "- A cloud-based inference flow (API key required)\n",
    "\n",
    "If you want to use **local models like LLaMA via Ollama**, you should consider using **LangChain**, which:\n",
    "- Supports `Ollama` as a local LLM backend\n",
    "- Allows more modular and customizable agent-tool combinations\n",
    "\n",
    "---\n",
    "\n",
    "### Agentic Flow Summary (CrewAI)\n",
    "\n",
    "| Component        | Description                                                                 |\n",
    "|------------------|-----------------------------------------------------------------------------|\n",
    "| **LLM**          | `ChatOpenAI` provides reasoning capabilities to all agents                  |\n",
    "| **Agents**       | Defined roles (Summarizer, QA Analyst) handle specific tasks independently  |\n",
    "| **Tasks**        | Contain scoped instructions and expected output, forming the task plan      |\n",
    "| **Crew**         | Orchestrates execution order and coordination between agents                |\n",
    "| **Mode**         | `sequential` process emulates multi-agent workflow                         |\n",
    "\n",
    "This setup offers a powerful example of **modular agent design** using declarative instructions — well suited for teaching autonomous behavior without needing to build complex control logic.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e13f91-874c-4003-9939-b29eb9273970",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1422078f-4a24-476f-b780-b12803f02460",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
