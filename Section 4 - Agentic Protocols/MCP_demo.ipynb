{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90b1d552-ae23-4128-b9e5-c177c8cc0a4e",
   "metadata": {},
   "source": [
    "#  MCP Exchange Demo: Agentic AI Communication with AutoGen and Ollama\n",
    "\n",
    "##  Goal\n",
    "This notebook demonstrates a **Message-Chain Protocol (MCP)**-style exchange between two agents using **AutoGen** and a **local LLM via Ollama**. The goal is to simulate how structured agent-to-agent communication unfolds in a transparent, replayable format with logs at each reasoning step.\n",
    "\n",
    "---\n",
    "\n",
    "##  Architecture\n",
    "- **`MCPUserProxyAgent`**: Simulates a user initiating a conversation and logging acknowledgments.\n",
    "- **`MCPAssistantAgent`**: Responds to the user using a locally hosted Ollama model (e.g., `llama3`) and logs its internal thought process and responses.\n",
    "- **MCP Logging**: Each message turn is logged with:\n",
    "  - `sender`, `receiver`\n",
    "  - `thought`, `observation`\n",
    "  - `timestamp` and `state_snapshot`\n",
    "  - `message_id` and `turn` number\n",
    "\n",
    "---\n",
    "\n",
    "##  Code Overview\n",
    "\n",
    "1. **`log_mcp_message()`**  \n",
    "   Captures and prints each structured MCP message including who said what, when, and with what reasoning.\n",
    "\n",
    "2. **LLM Configuration for Ollama**  \n",
    "   Provides a locally routed API interface to a model served with Ollama (e.g., `llama3`) using OpenAI-compatible endpoints.\n",
    "\n",
    "3. **`MCPAssistantAgent` Class**  \n",
    "   - Extends `AssistantAgent` to intercept and log all replies.\n",
    "   - Logs both incoming messages (thought) and generated responses (observation).\n",
    "\n",
    "4. **`MCPUserProxyAgent` Class**  \n",
    "   - Extends `UserProxyAgent` to simulate a user with one interaction round.\n",
    "   - Ends the conversation after acknowledging a response from the assistant.\n",
    "\n",
    "5. **Agent Instantiation**  \n",
    "   Creates the assistant and user proxy agents with appropriate parameters:\n",
    "   - Disables Docker\n",
    "   - Prevents prompting in Jupyter\n",
    "   - Enables default auto-reply\n",
    "\n",
    "6. **Conversation Execution**  \n",
    "   - Logs the initial user question\n",
    "   - Starts the interaction\n",
    "   - Captures final results or errors\n",
    "\n",
    "7. **MCP Log Display**  \n",
    "   Prints the entire exchange as structured JSON and a readable message summary.\n",
    "\n",
    "---\n",
    "\n",
    "##  Features Demonstrated\n",
    "- **Structured Thought-Action-Observation Reasoning**\n",
    "- **Replayable Message Logs**\n",
    "- **Turn-Based Coordination Between Agents**\n",
    "- **Integration with Local LLM (Ollama)**\n",
    "\n",
    "---\n",
    "\n",
    "##  Requirements\n",
    "- `autogen` Python package\n",
    "- Local Ollama model served via:\n",
    "  ```bash\n",
    "  ollama run llama3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c7e0b84-6a70-40a4-81bc-06b10dab3798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MCP Demo Exchange...\n",
      "============================================================\n",
      "\n",
      "--- MCP Message msg_001 ---\n",
      " 2025-06-20T17:31:51.610300\n",
      "From: user_proxy → To: climate_assistant\n",
      "Thought: Initiating conversation: What are two key points about global climate policy?\n",
      "------------------------------------------------------------\n",
      "\n",
      " User Query:\n",
      " What are two key points about global climate policy?\n",
      "----------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to climate_assistant):\n",
      "\n",
      "What are two key points about global climate policy?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- MCP Message msg_002 ---\n",
      " 2025-06-20T17:31:51.611680\n",
      "From: user_proxy → To: climate_assistant\n",
      "Thought: Processing request: What are two key points about global climate policy?\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- MCP Message msg_003 ---\n",
      " 2025-06-20T17:32:06.930326\n",
      "From: climate_assistant → To: user_proxy\n",
      "Thought: Generated response: Here are two key points about global climate policy:\n",
      "Observation:\n",
      "\n",
      "1. **Ratcheting up ambition is crucial**: The Paris Agreement's goal is to limit global warming to well below 2°C (3.6°F) and pursue efforts to limit it to 1.5°C (2.7°F). To achieve this, countries need to increase their Nationally Determined Contributions (NDCs) every five years. This \"ratcheting up\" of ambition is essential to avoid the most catastrophic impacts of climate change.\n",
      "\n",
      "2. **Carbon pricing and green finance can drive emissions cuts**: Implementing a global carbon price or equivalent mechanism, such as the EU's Emissions Trading System, can create an economic incentive for countries to reduce emissions. This can be complemented by mobilizing trillions of dollars in green finance through initiatives like the Green Climate Fund, which supports developing country climate action. Effective carbon pricing and financing can help drive the transition to a low-carbon economy and support equitable development.\n",
      "------------------------------------------------------------\n",
      "\n",
      " climate_assistant Response:\n",
      " Here are two key points about global climate policy:\n",
      "\n",
      "1. **Ratcheting up ambition is crucial**: The Paris Agreement's goal is to limit global warming to well below 2°C (3.6°F) and pursue efforts to limit it to 1.5°C (2.7°F). To achieve this, countries need to increase their Nationally Determined Contributions (NDCs) every five years. This \"ratcheting up\" of ambition is essential to avoid the most catastrophic impacts of climate change.\n",
      "\n",
      "2. **Carbon pricing and green finance can drive emissions cuts**: Implementing a global carbon price or equivalent mechanism, such as the EU's Emissions Trading System, can create an economic incentive for countries to reduce emissions. This can be complemented by mobilizing trillions of dollars in green finance through initiatives like the Green Climate Fund, which supports developing country climate action. Effective carbon pricing and financing can help drive the transition to a low-carbon economy and support equitable development.\n",
      "----------------------------------------\n",
      "\u001b[33mclimate_assistant\u001b[0m (to user_proxy):\n",
      "\n",
      "Here are two key points about global climate policy:\n",
      "\n",
      "1. **Ratcheting up ambition is crucial**: The Paris Agreement's goal is to limit global warming to well below 2°C (3.6°F) and pursue efforts to limit it to 1.5°C (2.7°F). To achieve this, countries need to increase their Nationally Determined Contributions (NDCs) every five years. This \"ratcheting up\" of ambition is essential to avoid the most catastrophic impacts of climate change.\n",
      "\n",
      "2. **Carbon pricing and green finance can drive emissions cuts**: Implementing a global carbon price or equivalent mechanism, such as the EU's Emissions Trading System, can create an economic incentive for countries to reduce emissions. This can be complemented by mobilizing trillions of dollars in green finance through initiatives like the Green Climate Fund, which supports developing country climate action. Effective carbon pricing and financing can help drive the transition to a low-carbon economy and support equitable development.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- MCP Message msg_004 ---\n",
      " 2025-06-20T17:32:06.936473\n",
      "From: user_proxy → To: climate_assistant\n",
      "Thought: Received and acknowledged: Here are two key points about global climate policy:\n",
      "Observation:\n",
      "\n",
      "1. **Ratcheting up ambition is crucial**: The ...\n",
      "------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to climate_assistant):\n",
      "\n",
      "Thank you for the information!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- MCP Message msg_005 ---\n",
      " 2025-06-20T17:32:06.939252\n",
      "From: user_proxy → To: climate_assistant\n",
      "Thought: Processing request: Thank you for the information!\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- MCP Message msg_006 ---\n",
      " 2025-06-20T17:32:10.057158\n",
      "From: climate_assistant → To: user_proxy\n",
      "Thought: Generated response: You're welcome! I'm glad I could provide you with some key takeaways on global climate policy. It's an important topic, and every effort counts in addressing the climate crisis. If you have any more questions or need further information, feel free to ask!\n",
      "------------------------------------------------------------\n",
      "\n",
      " climate_assistant Response:\n",
      " You're welcome! I'm glad I could provide you with some key takeaways on global climate policy. It's an important topic, and every effort counts in addressing the climate crisis. If you have any more questions or need further information, feel free to ask!\n",
      "----------------------------------------\n",
      "\u001b[33mclimate_assistant\u001b[0m (to user_proxy):\n",
      "\n",
      "You're welcome! I'm glad I could provide you with some key takeaways on global climate policy. It's an important topic, and every effort counts in addressing the climate crisis. If you have any more questions or need further information, feel free to ask!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- MCP Message msg_007 ---\n",
      " 2025-06-20T17:32:10.058834\n",
      "From: user_proxy → To: climate_assistant\n",
      "Thought: Demo exchange completed successfully\n",
      "------------------------------------------------------------\n",
      "\n",
      " Conversation completed successfully!\n",
      "\n",
      "============================================================\n",
      " Full MCP Exchange Log:\n",
      "============================================================\n",
      "[\n",
      "  {\n",
      "    \"message_id\": \"msg_001\",\n",
      "    \"timestamp\": \"2025-06-20T17:31:51.610300\",\n",
      "    \"turn\": 1,\n",
      "    \"sender\": \"user_proxy\",\n",
      "    \"receiver\": \"climate_assistant\",\n",
      "    \"thought\": \"Initiating conversation: What are two key points about global climate policy?\",\n",
      "    \"tool_call\": null,\n",
      "    \"tool_response\": null,\n",
      "    \"observation\": \"\",\n",
      "    \"state_snapshot\": {\n",
      "      \"is_response\": false,\n",
      "      \"conversation_stage\": \"Turn 1\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"message_id\": \"msg_002\",\n",
      "    \"timestamp\": \"2025-06-20T17:31:51.611680\",\n",
      "    \"turn\": 1,\n",
      "    \"sender\": \"user_proxy\",\n",
      "    \"receiver\": \"climate_assistant\",\n",
      "    \"thought\": \"Processing request: What are two key points about global climate policy?\",\n",
      "    \"tool_call\": null,\n",
      "    \"tool_response\": null,\n",
      "    \"observation\": \"\",\n",
      "    \"state_snapshot\": {\n",
      "      \"is_response\": false,\n",
      "      \"conversation_stage\": \"Turn 1\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"message_id\": \"msg_003\",\n",
      "    \"timestamp\": \"2025-06-20T17:32:06.930326\",\n",
      "    \"turn\": 2,\n",
      "    \"sender\": \"climate_assistant\",\n",
      "    \"receiver\": \"user_proxy\",\n",
      "    \"thought\": \"Generated response: Here are two key points about global climate policy:\",\n",
      "    \"tool_call\": null,\n",
      "    \"tool_response\": null,\n",
      "    \"observation\": \"\\n1. **Ratcheting up ambition is crucial**: The Paris Agreement's goal is to limit global warming to well below 2\\u00b0C (3.6\\u00b0F) and pursue efforts to limit it to 1.5\\u00b0C (2.7\\u00b0F). To achieve this, countries need to increase their Nationally Determined Contributions (NDCs) every five years. This \\\"ratcheting up\\\" of ambition is essential to avoid the most catastrophic impacts of climate change.\\n\\n2. **Carbon pricing and green finance can drive emissions cuts**: Implementing a global carbon price or equivalent mechanism, such as the EU's Emissions Trading System, can create an economic incentive for countries to reduce emissions. This can be complemented by mobilizing trillions of dollars in green finance through initiatives like the Green Climate Fund, which supports developing country climate action. Effective carbon pricing and financing can help drive the transition to a low-carbon economy and support equitable development.\",\n",
      "    \"state_snapshot\": {\n",
      "      \"is_response\": true,\n",
      "      \"conversation_stage\": \"Turn 2\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"message_id\": \"msg_004\",\n",
      "    \"timestamp\": \"2025-06-20T17:32:06.936473\",\n",
      "    \"turn\": 2,\n",
      "    \"sender\": \"user_proxy\",\n",
      "    \"receiver\": \"climate_assistant\",\n",
      "    \"thought\": \"Received and acknowledged: Here are two key points about global climate policy:\",\n",
      "    \"tool_call\": null,\n",
      "    \"tool_response\": null,\n",
      "    \"observation\": \"\\n1. **Ratcheting up ambition is crucial**: The ...\",\n",
      "    \"state_snapshot\": {\n",
      "      \"is_response\": true,\n",
      "      \"conversation_stage\": \"Turn 2\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"message_id\": \"msg_005\",\n",
      "    \"timestamp\": \"2025-06-20T17:32:06.939252\",\n",
      "    \"turn\": 3,\n",
      "    \"sender\": \"user_proxy\",\n",
      "    \"receiver\": \"climate_assistant\",\n",
      "    \"thought\": \"Processing request: Thank you for the information!\",\n",
      "    \"tool_call\": null,\n",
      "    \"tool_response\": null,\n",
      "    \"observation\": \"\",\n",
      "    \"state_snapshot\": {\n",
      "      \"is_response\": false,\n",
      "      \"conversation_stage\": \"Turn 3\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"message_id\": \"msg_006\",\n",
      "    \"timestamp\": \"2025-06-20T17:32:10.057158\",\n",
      "    \"turn\": 3,\n",
      "    \"sender\": \"climate_assistant\",\n",
      "    \"receiver\": \"user_proxy\",\n",
      "    \"thought\": \"Generated response: You're welcome! I'm glad I could provide you with some key takeaways on global climate policy. It's an important topic, and every effort counts in addressing the climate crisis. If you have any more questions or need further information, feel free to ask!\",\n",
      "    \"tool_call\": null,\n",
      "    \"tool_response\": null,\n",
      "    \"observation\": \"\",\n",
      "    \"state_snapshot\": {\n",
      "      \"is_response\": true,\n",
      "      \"conversation_stage\": \"Turn 3\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"message_id\": \"msg_007\",\n",
      "    \"timestamp\": \"2025-06-20T17:32:10.058834\",\n",
      "    \"turn\": 4,\n",
      "    \"sender\": \"user_proxy\",\n",
      "    \"receiver\": \"climate_assistant\",\n",
      "    \"thought\": \"Demo exchange completed successfully\",\n",
      "    \"tool_call\": null,\n",
      "    \"tool_response\": null,\n",
      "    \"observation\": \"\",\n",
      "    \"state_snapshot\": {\n",
      "      \"is_response\": true,\n",
      "      \"conversation_stage\": \"Turn 4\"\n",
      "    }\n",
      "  }\n",
      "]\n",
      "\n",
      " Total MCP messages logged: 7\n",
      " MCP Protocol Exchange Summary:\n",
      "  1. user_proxy → climate_assistant: Initiating conversation: What are two key points a...\n",
      "  2. user_proxy → climate_assistant: Processing request: What are two key points about ...\n",
      "  3. climate_assistant → user_proxy: Generated response: Here are two key points about ...\n",
      "  4. user_proxy → climate_assistant: Received and acknowledged: Here are two key points...\n",
      "  5. user_proxy → climate_assistant: Processing request: Thank you for the information!...\n",
      "  6. climate_assistant → user_proxy: Generated response: You're welcome! I'm glad I cou...\n",
      "  7. user_proxy → climate_assistant: Demo exchange completed successfully...\n",
      "\n",
      " Demo completed!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "# Agentic AI Demo: MCP-Style Exchange Between Two Agents\n",
    "# =============================================================\n",
    "\n",
    "# Imports for structured messaging and time tracking\n",
    "import json\n",
    "from datetime import datetime\n",
    "from autogen import AssistantAgent, UserProxyAgent\n",
    "\n",
    "# === GLOBALS ===\n",
    "\n",
    "# Stores the log of all MCP messages (thought, action, observation)\n",
    "mcp_log = []\n",
    "\n",
    "# Tracks number of messages exchanged in total\n",
    "message_counter = 0\n",
    "\n",
    "# === MCP Logging Function ===\n",
    "\n",
    "def log_mcp_message(msg_id, sender, receiver, content, turn, is_response=False):\n",
    "    \"\"\"\n",
    "    Records a structured MCP message with metadata including\n",
    "    thought, observation, sender, receiver, and timestamp.\n",
    "    \"\"\"\n",
    "    now = datetime.utcnow().isoformat()\n",
    "    lines = content.strip().splitlines() if content else [\"\"]\n",
    "\n",
    "    # First line is treated as the agent's 'thought'\n",
    "    thought = lines[0] if lines else \"\"\n",
    "    # Remaining lines are considered the agent's 'observation'\n",
    "    observation = \"\\n\".join(lines[1:]) if len(lines) > 1 else \"\"\n",
    "\n",
    "    # Construct the MCP message dictionary\n",
    "    mcp_entry = {\n",
    "        \"message_id\": f\"msg_{msg_id:03}\",\n",
    "        \"timestamp\": now,\n",
    "        \"turn\": turn,\n",
    "        \"sender\": sender,\n",
    "        \"receiver\": receiver,\n",
    "        \"thought\": thought,\n",
    "        \"tool_call\": None,         # Placeholder; could log tool name here\n",
    "        \"tool_response\": None,     # Placeholder; could log tool output here\n",
    "        \"observation\": observation,\n",
    "        \"state_snapshot\": {\n",
    "            \"is_response\": is_response,\n",
    "            \"conversation_stage\": f\"Turn {turn}\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Append the entry to the global MCP log\n",
    "    mcp_log.append(mcp_entry)\n",
    "\n",
    "    # Print the message in a readable format\n",
    "    print(f\"\\n--- MCP Message {mcp_entry['message_id']} ---\")\n",
    "    print(f\" {now}\")\n",
    "    print(f\"From: {sender} → To: {receiver}\")\n",
    "    print(f\"Thought: {thought}\")\n",
    "    if observation:\n",
    "        print(f\"Observation:\\n{observation}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# === Local LLM Configuration for Ollama ===\n",
    "\n",
    "llm_config = {\n",
    "    \"config_list\": [\n",
    "        {\n",
    "            \"model\": \"llama3\",  # Update to match the Ollama model you are running\n",
    "            \"base_url\": \"http://localhost:11434/v1\",  # Ollama default endpoint\n",
    "            \"api_key\": \"ollama\",                      # Dummy key for compatibility\n",
    "            \"price\": [0.0, 0.0]                        # Avoids cost warnings\n",
    "        }\n",
    "    ],\n",
    "    \"timeout\": 60,\n",
    "}\n",
    "\n",
    "# === MCP-Enabled Assistant Agent ===\n",
    "\n",
    "class MCPAssistantAgent(AssistantAgent):\n",
    "    \"\"\"\n",
    "    Wraps AssistantAgent to log each incoming and outgoing message as an MCP message.\n",
    "    \"\"\"\n",
    "    def generate_reply(self, messages=None, sender=None, config=None):\n",
    "        global message_counter\n",
    "        if messages is None:\n",
    "            messages = []\n",
    "\n",
    "        # Log the incoming message (agent's 'thought')\n",
    "        if messages:\n",
    "            content = messages[-1].get(\"content\", \"\") if isinstance(messages[-1], dict) else str(messages[-1])\n",
    "            message_counter += 1\n",
    "            log_mcp_message(\n",
    "                msg_id=message_counter,\n",
    "                sender=sender.name if sender and hasattr(sender, 'name') else \"unknown\",\n",
    "                receiver=self.name,\n",
    "                content=f\"Processing request: {content}\",\n",
    "                turn=(message_counter + 1) // 2,\n",
    "                is_response=False\n",
    "            )\n",
    "\n",
    "        # Generate actual response using parent class logic\n",
    "        try:\n",
    "            response = super().generate_reply(messages, sender, config)\n",
    "        except TypeError:\n",
    "            # Handles backwards compatibility for older AutoGen versions\n",
    "            try:\n",
    "                response = super().generate_reply(messages, sender)\n",
    "            except:\n",
    "                response = super().generate_reply(messages)\n",
    "\n",
    "        # Extract the response content for logging\n",
    "        message_counter += 1\n",
    "        if isinstance(response, dict):\n",
    "            response_content = response.get(\"content\", \"\")\n",
    "        elif isinstance(response, str):\n",
    "            response_content = response\n",
    "        else:\n",
    "            response_content = str(response) if response else \"No response generated\"\n",
    "\n",
    "        # Log the assistant's response (observation)\n",
    "        log_mcp_message(\n",
    "            msg_id=message_counter,\n",
    "            sender=self.name,\n",
    "            receiver=sender.name if sender and hasattr(sender, 'name') else \"unknown\",\n",
    "            content=f\"Generated response: {response_content}\",\n",
    "            turn=(message_counter + 1) // 2,\n",
    "            is_response=True\n",
    "        )\n",
    "\n",
    "        # Print the assistant's response to console\n",
    "        print(f\"\\n {self.name} Response:\")\n",
    "        print(f\" {response_content}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        return response\n",
    "\n",
    "# === MCP-Enabled User Agent ===\n",
    "\n",
    "class MCPUserProxyAgent(UserProxyAgent):\n",
    "    \"\"\"\n",
    "    Wraps UserProxyAgent to automatically log responses and simulate a single turn.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.response_count = 0\n",
    "\n",
    "    def generate_reply(self, messages=None, sender=None, config=None):\n",
    "        global message_counter\n",
    "        if messages is None:\n",
    "            messages = []\n",
    "\n",
    "        self.response_count += 1\n",
    "\n",
    "        # Stop after 1 assistant response for demonstration purposes\n",
    "        if self.response_count > 1:\n",
    "            message_counter += 1\n",
    "            log_mcp_message(\n",
    "                msg_id=message_counter,\n",
    "                sender=self.name,\n",
    "                receiver=sender.name if sender and hasattr(sender, 'name') else \"unknown\",\n",
    "                content=\"Demo exchange completed successfully\",\n",
    "                turn=(message_counter + 1) // 2,\n",
    "                is_response=True\n",
    "            )\n",
    "            return None\n",
    "\n",
    "        # Log the assistant message being processed\n",
    "        if messages:\n",
    "            last_message = messages[-1]\n",
    "            content = last_message.get(\"content\", \"\") if isinstance(last_message, dict) else str(last_message)\n",
    "            message_counter += 1\n",
    "            log_mcp_message(\n",
    "                msg_id=message_counter,\n",
    "                sender=self.name,\n",
    "                receiver=sender.name if sender and hasattr(sender, 'name') else \"unknown\",\n",
    "                content=f\"Received and acknowledged: {content[:100]}...\",\n",
    "                turn=(message_counter + 1) // 2,\n",
    "                is_response=True\n",
    "            )\n",
    "\n",
    "        # Continue the conversation (will be auto-replied)\n",
    "        try:\n",
    "            return super().generate_reply(messages, sender, config)\n",
    "        except TypeError:\n",
    "            try:\n",
    "                return super().generate_reply(messages, sender)\n",
    "            except:\n",
    "                return super().generate_reply(messages)\n",
    "\n",
    "# === Instantiate the Agents ===\n",
    "\n",
    "user_proxy = MCPUserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    human_input_mode=\"NEVER\",                      # Prevent interactive prompts\n",
    "    max_consecutive_auto_reply=1,                  # Allow one auto-response\n",
    "    code_execution_config=False,                   # No Docker execution needed\n",
    "    default_auto_reply=\"Thank you for the information!\"\n",
    ")\n",
    "\n",
    "assistant = MCPAssistantAgent(\n",
    "    name=\"climate_assistant\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"You are a helpful assistant focused on climate policy. Provide exactly two key points about the topic in a clear, concise manner.\"\n",
    ")\n",
    "\n",
    "# === Run the MCP Exchange ===\n",
    "\n",
    "print(\"Starting MCP Demo Exchange...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Step 1: Log the user's initial request\n",
    "    initial_message = \"What are two key points about global climate policy?\"\n",
    "    message_counter += 1\n",
    "    log_mcp_message(\n",
    "        msg_id=message_counter,\n",
    "        sender=\"user_proxy\",\n",
    "        receiver=\"climate_assistant\",\n",
    "        content=f\"Initiating conversation: {initial_message}\",\n",
    "        turn=1,\n",
    "        is_response=False\n",
    "    )\n",
    "\n",
    "    print(f\"\\n User Query:\")\n",
    "    print(f\" {initial_message}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # Step 2: Start the conversation\n",
    "    result = user_proxy.initiate_chat(\n",
    "        recipient=assistant,\n",
    "        message=initial_message,\n",
    "        max_turns=3,\n",
    "        silent=False\n",
    "    )\n",
    "\n",
    "    print(\"\\n Conversation completed successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    # If anything fails, show helpful diagnostic message\n",
    "    print(f\" Error during chat: {e}\")\n",
    "    print(\"This might be due to:\")\n",
    "    print(\"- Ollama not running (try: ollama serve)\")\n",
    "    print(\"- Model not available (try: ollama pull llama3)\")\n",
    "    print(\"- Network connectivity issues\")\n",
    "    print(\"- AutoGen version compatibility\")\n",
    "\n",
    "# === Show MCP Message Log ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\" Full MCP Exchange Log:\")\n",
    "print(\"=\" * 60)\n",
    "print(json.dumps(mcp_log, indent=2))\n",
    "\n",
    "print(f\"\\n Total MCP messages logged: {len(mcp_log)}\")\n",
    "print(\" MCP Protocol Exchange Summary:\")\n",
    "for i, entry in enumerate(mcp_log, 1):\n",
    "    print(f\"  {i}. {entry['sender']} → {entry['receiver']}: {entry['thought'][:50]}...\")\n",
    "\n",
    "print(\"\\n Demo completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96430da7-263e-47c0-b3c6-57603dd75bb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f07e1b-1c01-4e10-ad1e-791cfacc192c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04b5467f-abdb-40eb-a1a2-9aa054f99ac8",
   "metadata": {},
   "source": [
    "##  Demonstration: Tool-Calling Assistant using Model Context Protocol (MCP)\n",
    "\n",
    "This segment of the notebook demonstrates how to use the **Model Context Protocol (MCP)** format with OpenAI `gpt-4-0613` model to simulate a tool-using assistant.\n",
    "\n",
    "###  Objective\n",
    "The assistant is asked a simple math question:  \n",
    "**“What is the sum of 12 and 30?”**  \n",
    "The assistant is allowed to call a custom-defined tool (`get_sum`) to compute the answer.\n",
    "\n",
    "---\n",
    "\n",
    "###  Code Breakdown\n",
    "\n",
    "#### **Step 1: Load API key**\n",
    "Loads the OpenAI API key from a `.env` file using `dotenv`.\n",
    "\n",
    "#### **Step 2: Define the Tool**\n",
    "A tool named `get_sum` is declared. It:\n",
    "- Takes two numbers `a` and `b`\n",
    "- Returns their sum\n",
    "\n",
    "This tool is made available to the model via the `tools` argument.\n",
    "\n",
    "#### **Step 3: Create the Message History**\n",
    "The conversation starts with a `system` prompt (setting assistant behavior), followed by a `user` question:  \n",
    "_\"What's the sum of 12 and 30?\"_\n",
    "\n",
    "#### **Step 4: First Model Call**\n",
    "The assistant receives the user input and decides whether to:\n",
    "- Answer directly, **or**\n",
    "- Use a tool (in this case, `get_sum`)  \n",
    "\n",
    "The tool choice is left to the model using `tool_choice=\"auto\"`.\n",
    "\n",
    "#### **Step 5: Process the Model’s Decision**\n",
    "- If a tool call is returned, it’s printed (function name, arguments, and ID).\n",
    "- If not, the assistant's direct response is printed.\n",
    "\n",
    "#### **Step 6: Simulate the Tool Execution**\n",
    "We simulate the actual tool execution by:\n",
    "- Parsing the arguments with `json.loads`\n",
    "- Computing the result (`a + b`)\n",
    "- Packaging the result as a response to the assistant\n",
    "\n",
    "#### **Step 7: Extend the Message History**\n",
    "The tool call and its result are appended to the conversation. This mirrors what would happen if a backend system executed the tool and returned the output to the LLM.\n",
    "\n",
    "#### **Step 8: Second Model Call**\n",
    "The assistant sees the tool’s result and responds to the user. This step closes the loop with a natural-language answer.\n",
    "\n",
    "#### **Step 9: Print Final Answer**\n",
    "Displays the final assistant response, demonstrating how the LLM:\n",
    "- Called a tool\n",
    "- Interpreted the result\n",
    "- Answered the original question\n",
    "\n",
    "---\n",
    "\n",
    "###  Summary of MCP Interaction\n",
    "\n",
    "1. Assistant receives the user query\n",
    "2. Assistant decides to call the `get_sum` tool\n",
    "3. Tool returns: `{ \"result\": 42 }`\n",
    "4. Assistant replies:  \n",
    "   **“The sum of 12 and 30 is 42.”**\n",
    "\n",
    "This workflow showcases **multi-turn structured interactions** via **MCP**, allowing for **tool use**, **state tracking**, and **transparent reasoning**.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1664942c-2968-455c-b121-5c0ee94d71a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ MCP MESSAGE 1 =================\n",
      " Assistant receives the user message and evaluates:\n",
      "   → 'What's the sum of 12 and 30?'\n",
      " Assistant decides to use a tool (function call)\n",
      " Tool Call ID: call_8xfGpvGMOoGmC3l8dUZjA4hw\n",
      " Tool Name:   get_sum\n",
      " Arguments:   {\n",
      "  \"a\": 12,\n",
      "  \"b\": 30\n",
      "}\n",
      "\n",
      " Executing Tool Function:\n",
      "→ get_sum(12, 30) = 42\n",
      "\n",
      "================ MCP MESSAGE 2 =================\n",
      " Assistant receives the tool result:\n",
      "   → {'result': 42}\n",
      " Responds to the user:\n",
      "   → The sum of 12 and 30 is 42.\n",
      "\n",
      "================ SUMMARY =================\n",
      " MCP Conversation complete:\n",
      "   1. Assistant received a question\n",
      "   2. Decided to call 'get_sum'\n",
      "   3. Tool returned result = 42\n",
      "   4. Assistant responded with the final answer\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "# === STEP 1: Load OpenAI API key from local .env file ===\n",
    "load_dotenv(\"keys.env\")\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# === STEP 2: Define available tool(s) ===\n",
    "# The assistant can use this tool if it determines that it needs to perform a computation\n",
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_sum\",\n",
    "        \"description\": \"Returns the sum of two numbers\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"a\": {\"type\": \"number\", \"description\": \"First number\"},\n",
    "                \"b\": {\"type\": \"number\", \"description\": \"Second number\"}\n",
    "            },\n",
    "            \"required\": [\"a\", \"b\"]\n",
    "        }\n",
    "    }\n",
    "}]\n",
    "\n",
    "# === STEP 3: Construct the initial conversation ===\n",
    "# The user is asking a simple math question.\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that uses tools to compute results.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the sum of 12 and 30?\"}\n",
    "]\n",
    "\n",
    "# === STEP 4: First model call — LLM decides if it needs a tool ===\n",
    "response_1 = client.chat.completions.create(\n",
    "    model=\"gpt-4-0613\",\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\"  # Allow the model to decide\n",
    ")\n",
    "\n",
    "mcp_msg_1 = response_1.choices[0].message\n",
    "\n",
    "# === STEP 5: Display MCP Message 1 (Tool Call Decision) ===\n",
    "print(\"\\n================ MCP MESSAGE 1 =================\")\n",
    "print(\" Assistant receives the user message and evaluates:\")\n",
    "print(\"   → 'What's the sum of 12 and 30?'\")\n",
    "\n",
    "if mcp_msg_1.tool_calls:\n",
    "    print(\" Assistant decides to use a tool (function call)\")\n",
    "    tool_call = mcp_msg_1.tool_calls[0]\n",
    "    print(\" Tool Call ID:\", tool_call.id)\n",
    "    print(\" Tool Name:  \", tool_call.function.name)\n",
    "    print(\" Arguments:  \", tool_call.function.arguments)\n",
    "else:\n",
    "    print(\" No tool call detected\")\n",
    "    print(\"Response content:\", mcp_msg_1.content)\n",
    "\n",
    "# === STEP 6: Simulate executing the tool (e.g., backend function) ===\n",
    "# The assistant requested: get_sum(a=12, b=30)\n",
    "# We now simulate executing this locally in our Python environment\n",
    "tool_call = mcp_msg_1.tool_calls[0]\n",
    "tool_args = json.loads(tool_call.function.arguments)  # Safer than eval\n",
    "result = tool_args[\"a\"] + tool_args[\"b\"]\n",
    "tool_result = {\"result\": result}\n",
    "\n",
    "print(\"\\n Executing Tool Function:\")\n",
    "print(f\"→ get_sum({tool_args['a']}, {tool_args['b']}) = {result}\")\n",
    "\n",
    "# === STEP 7: Add tool call + result to message history ===\n",
    "# This is how we inform the assistant what the tool returned\n",
    "messages.append(mcp_msg_1)  # Add the tool call message\n",
    "messages.append({\n",
    "    \"role\": \"tool\",\n",
    "    \"tool_call_id\": tool_call.id,\n",
    "    \"content\": json.dumps(tool_result)\n",
    "})\n",
    "\n",
    "# === STEP 8: Second model call — LLM integrates tool result and responds ===\n",
    "response_2 = client.chat.completions.create(\n",
    "    model=\"gpt-4-0613\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "final_response = response_2.choices[0].message.content\n",
    "\n",
    "# === STEP 9: Display MCP Message 2 (Final Assistant Response) ===\n",
    "print(\"\\n================ MCP MESSAGE 2 =================\")\n",
    "print(\" Assistant receives the tool result:\")\n",
    "print(\"   → {'result': 42}\")\n",
    "print(\" Responds to the user:\")\n",
    "print(\"   →\", final_response)\n",
    "\n",
    "# === Summary ===\n",
    "print(\"\\n================ SUMMARY =================\")\n",
    "print(\" MCP Conversation complete:\")\n",
    "print(\"   1. Assistant received a question\")\n",
    "print(\"   2. Decided to call 'get_sum'\")\n",
    "print(\"   3. Tool returned result = 42\")\n",
    "print(\"   4. Assistant responded with the final answer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0460457e-3566-4fc5-994e-7d9353cfb025",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
