{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f1c5833-e478-477d-a064-a7e0084b2e6b",
   "metadata": {},
   "source": [
    "##  Demonstration: Tool-Calling Assistant using Model Context Protocol (MCP)\n",
    "\n",
    "This segment of the notebook demonstrates how to use the **Model Context Protocol (MCP)** format with OpenAI `gpt-4-0613` model to simulate a tool-using assistant.\n",
    "\n",
    "###  Objective\n",
    "The assistant is asked a simple math question:  \n",
    "**“What is the sum of 12 and 30?”**  \n",
    "The assistant is allowed to call a custom-defined tool (`get_sum`) to compute the answer.\n",
    "\n",
    "---\n",
    "\n",
    "###  Code Breakdown\n",
    "\n",
    "#### **Step 1: Load API key**\n",
    "Loads the OpenAI API key from a `.env` file using `dotenv`.\n",
    "\n",
    "#### **Step 2: Define the Tool**\n",
    "A tool named `get_sum` is declared. It:\n",
    "- Takes two numbers `a` and `b`\n",
    "- Returns their sum\n",
    "\n",
    "This tool is made available to the model via the `tools` argument.\n",
    "\n",
    "#### **Step 3: Create the Message History**\n",
    "The conversation starts with a `system` prompt (setting assistant behavior), followed by a `user` question:  \n",
    "_\"What's the sum of 12 and 30?\"_\n",
    "\n",
    "#### **Step 4: First Model Call**\n",
    "The assistant receives the user input and decides whether to:\n",
    "- Answer directly, **or**\n",
    "- Use a tool (in this case, `get_sum`)  \n",
    "\n",
    "The tool choice is left to the model using `tool_choice=\"auto\"`.\n",
    "\n",
    "#### **Step 5: Process the Model’s Decision**\n",
    "- If a tool call is returned, it’s printed (function name, arguments, and ID).\n",
    "- If not, the assistant's direct response is printed.\n",
    "\n",
    "#### **Step 6: Simulate the Tool Execution**\n",
    "We simulate the actual tool execution by:\n",
    "- Parsing the arguments with `json.loads`\n",
    "- Computing the result (`a + b`)\n",
    "- Packaging the result as a response to the assistant\n",
    "\n",
    "#### **Step 7: Extend the Message History**\n",
    "The tool call and its result are appended to the conversation. This mirrors what would happen if a backend system executed the tool and returned the output to the LLM.\n",
    "\n",
    "#### **Step 8: Second Model Call**\n",
    "The assistant sees the tool’s result and responds to the user. This step closes the loop with a natural-language answer.\n",
    "\n",
    "#### **Step 9: Print Final Answer**\n",
    "Displays the final assistant response, demonstrating how the LLM:\n",
    "- Called a tool\n",
    "- Interpreted the result\n",
    "- Answered the original question\n",
    "\n",
    "---\n",
    "\n",
    "###  Summary of MCP Interaction\n",
    "\n",
    "1. Assistant receives the user query\n",
    "2. Assistant decides to call the `get_sum` tool\n",
    "3. Tool returns: `{ \"result\": 42 }`\n",
    "4. Assistant replies:  \n",
    "   **“The sum of 12 and 30 is 42.”**\n",
    "\n",
    "This workflow showcases **multi-turn structured interactions** via **MCP**, allowing for **tool use**, **state tracking**, and **transparent reasoning**.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1664942c-2968-455c-b121-5c0ee94d71a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ Model Context Protocol MESSAGE 1 =================\n",
      " Assistant receives the user message and evaluates:\n",
      "   → 'What's the sum of 12 and 30?'\n",
      " Assistant decides to use a tool (function call)\n",
      " Tool Call ID: call_JKzoHu9GnEFt9Lstv4tyAjlP\n",
      " Tool Name:   get_sum\n",
      " Arguments:   {\n",
      "  \"a\": 12,\n",
      "  \"b\": 30\n",
      "}\n",
      "\n",
      " Executing Tool Function:\n",
      "→ get_sum(12, 30) = 42\n",
      "\n",
      "================ Model Context Protocol MESSAGE 2 =================\n",
      " Assistant receives the tool result:\n",
      "   → {'result': 42}\n",
      " Responds to the user:\n",
      "   → The sum of 12 and 30 is 42.\n",
      "\n",
      "================ SUMMARY =================\n",
      " MCP Conversation complete:\n",
      "   1. Assistant received a question\n",
      "   2. Decided to call 'get_sum'\n",
      "   3. Tool returned result = 42\n",
      "   4. Assistant responded with the final answer\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "# === STEP 1: Load OpenAI API key from local .env file ===\n",
    "load_dotenv(\"keys.env\")\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# === STEP 2: Define available tool(s) ===\n",
    "# The assistant can use this tool if it determines that it needs to perform a computation\n",
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_sum\",\n",
    "        \"description\": \"Returns the sum of two numbers\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"a\": {\"type\": \"number\", \"description\": \"First number\"},\n",
    "                \"b\": {\"type\": \"number\", \"description\": \"Second number\"}\n",
    "            },\n",
    "            \"required\": [\"a\", \"b\"]\n",
    "        }\n",
    "    }\n",
    "}]\n",
    "\n",
    "# === STEP 3: Construct the initial conversation ===\n",
    "# The user is asking a simple math question.\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that uses tools to compute results.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the sum of 12 and 30?\"}\n",
    "]\n",
    "\n",
    "# === STEP 4: First model call — LLM decides if it needs a tool ===\n",
    "response_1 = client.chat.completions.create(\n",
    "    model=\"gpt-4-0613\",\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\"  # Allow the model to decide\n",
    ")\n",
    "\n",
    "mcp_msg_1 = response_1.choices[0].message\n",
    "\n",
    "# === STEP 5: Display MCP Message 1 (Tool Call Decision) ===\n",
    "print(\"\\n================ Model Context Protocol MESSAGE 1 =================\")\n",
    "print(\" Assistant receives the user message and evaluates:\")\n",
    "print(\"   → 'What's the sum of 12 and 30?'\")\n",
    "\n",
    "if mcp_msg_1.tool_calls:\n",
    "    print(\" Assistant decides to use a tool (function call)\")\n",
    "    tool_call = mcp_msg_1.tool_calls[0]\n",
    "    print(\" Tool Call ID:\", tool_call.id)\n",
    "    print(\" Tool Name:  \", tool_call.function.name)\n",
    "    print(\" Arguments:  \", tool_call.function.arguments)\n",
    "else:\n",
    "    print(\" No tool call detected\")\n",
    "    print(\"Response content:\", mcp_msg_1.content)\n",
    "\n",
    "# === STEP 6: Simulate executing the tool (e.g., backend function) ===\n",
    "# The assistant requested: get_sum(a=12, b=30)\n",
    "# We now simulate executing this locally in our Python environment\n",
    "tool_call = mcp_msg_1.tool_calls[0]\n",
    "tool_args = json.loads(tool_call.function.arguments)  # Safer than eval\n",
    "result = tool_args[\"a\"] + tool_args[\"b\"]\n",
    "tool_result = {\"result\": result}\n",
    "\n",
    "print(\"\\n Executing Tool Function:\")\n",
    "print(f\"→ get_sum({tool_args['a']}, {tool_args['b']}) = {result}\")\n",
    "\n",
    "# === STEP 7: Add tool call + result to message history ===\n",
    "# This is how we inform the assistant what the tool returned\n",
    "messages.append(mcp_msg_1)  # Add the tool call message\n",
    "messages.append({\n",
    "    \"role\": \"tool\",\n",
    "    \"tool_call_id\": tool_call.id,\n",
    "    \"content\": json.dumps(tool_result)\n",
    "})\n",
    "\n",
    "# === STEP 8: Second model call — LLM integrates tool result and responds ===\n",
    "response_2 = client.chat.completions.create(\n",
    "    model=\"gpt-4-0613\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "final_response = response_2.choices[0].message.content\n",
    "\n",
    "# === STEP 9: Display MCP Message 2 (Final Assistant Response) ===\n",
    "print(\"\\n================ Model Context Protocol MESSAGE 2 =================\")\n",
    "print(\" Assistant receives the tool result:\")\n",
    "print(\"   → {'result': 42}\")\n",
    "print(\" Responds to the user:\")\n",
    "print(\"   →\", final_response)\n",
    "\n",
    "# === Summary ===\n",
    "print(\"\\n================ SUMMARY =================\")\n",
    "print(\" MCP Conversation complete:\")\n",
    "print(\"   1. Assistant received a question\")\n",
    "print(\"   2. Decided to call 'get_sum'\")\n",
    "print(\"   3. Tool returned result = 42\")\n",
    "print(\"   4. Assistant responded with the final answer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca381390-1066-4eae-b762-13d27f0a13b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90b1d552-ae23-4128-b9e5-c177c8cc0a4e",
   "metadata": {},
   "source": [
    "#  Message Chain Protocol Exchange Demo: Agentic AI Communication with AutoGen and Ollama\n",
    "\n",
    "##  Goal\n",
    "This section of the notebook demonstrates a **Message-Chain Protocol (MCP)**-style exchange between two agents using **AutoGen** and a **local LLM via Ollama**. The goal is to simulate how structured agent-to-agent communication unfolds in a transparent, replayable format with logs at each reasoning step.\n",
    "\n",
    "---\n",
    "\n",
    "##  Architecture\n",
    "- **`MCPUserProxyAgent`**: Simulates a user initiating a conversation and logging acknowledgments.\n",
    "- **`MCPAssistantAgent`**: Responds to the user using a locally hosted Ollama model (e.g., `llama3`) and logs its internal thought process and responses.\n",
    "- **MCP Logging**: Each message turn is logged with:\n",
    "  - `sender`, `receiver`\n",
    "  - `thought`, `observation`\n",
    "  - `timestamp` and `state_snapshot`\n",
    "  - `message_id` and `turn` number\n",
    "\n",
    "---\n",
    "\n",
    "##  Code Overview\n",
    "\n",
    "1. **`log_mcp_message()`**  \n",
    "   Captures and prints each structured MCP message including who said what, when, and with what reasoning.\n",
    "\n",
    "2. **LLM Configuration for Ollama**  \n",
    "   Provides a locally routed API interface to a model served with Ollama (e.g., `llama3`) using OpenAI-compatible endpoints.\n",
    "\n",
    "3. **`MCPAssistantAgent` Class**  \n",
    "   - Extends `AssistantAgent` to intercept and log all replies.\n",
    "   - Logs both incoming messages (thought) and generated responses (observation).\n",
    "\n",
    "4. **`MCPUserProxyAgent` Class**  \n",
    "   - Extends `UserProxyAgent` to simulate a user with one interaction round.\n",
    "   - Ends the conversation after acknowledging a response from the assistant.\n",
    "\n",
    "5. **Agent Instantiation**  \n",
    "   Creates the assistant and user proxy agents with appropriate parameters:\n",
    "   - Disables Docker\n",
    "   - Prevents prompting in Jupyter\n",
    "   - Enables default auto-reply\n",
    "\n",
    "6. **Conversation Execution**  \n",
    "   - Logs the initial user question\n",
    "   - Starts the interaction\n",
    "   - Captures final results or errors\n",
    "\n",
    "7. **MCP Log Display**  \n",
    "   Prints the entire exchange as structured JSON and a readable message summary.\n",
    "\n",
    "---\n",
    "\n",
    "##  Features Demonstrated\n",
    "- **Structured Thought-Action-Observation Reasoning**\n",
    "- **Replayable Message Logs**\n",
    "- **Turn-Based Coordination Between Agents**\n",
    "- **Integration with Local LLM (Ollama)**\n",
    "\n",
    "---\n",
    "\n",
    "##  Requirements\n",
    "- `autogen` Python package\n",
    "- Local Ollama model served via:\n",
    "  ```bash\n",
    "  ollama run llama3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c7e0b84-6a70-40a4-81bc-06b10dab3798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Message Chain Protocol Demo Exchange...\n",
      "============================================================\n",
      "\n",
      "--- Message Chain Protocol Message msg_001 ---\n",
      " 2025-06-21T15:15:40.538940\n",
      "From: user_proxy → To: climate_assistant\n",
      "Thought: Initiating conversation: What are two key points about global climate policy?\n",
      "------------------------------------------------------------\n",
      "\n",
      " User Query:\n",
      " What are two key points about global climate policy?\n",
      "----------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to climate_assistant):\n",
      "\n",
      "What are two key points about global climate policy?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Message Chain Protocol Message msg_002 ---\n",
      " 2025-06-21T15:15:40.540212\n",
      "From: user_proxy → To: climate_assistant\n",
      "Thought: Processing request: What are two key points about global climate policy?\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Message Chain Protocol Message msg_003 ---\n",
      " 2025-06-21T15:15:58.872625\n",
      "From: climate_assistant → To: user_proxy\n",
      "Thought: Generated response: Here are two key points about global climate policy:\n",
      "Observation:\n",
      "\n",
      "**Point 1: Staying under 1.5°C is crucial**: The Paris Agreement sets an ambitious goal of limiting global warming to well below 2°C (3.6°F) and pursue efforts to limit it to 1.5°C (2.7°F) above pre-industrial levels. This threshold is critical because it would help avoid the most catastrophic impacts of climate change, such as sea-level rise, droughts, and extreme weather events. To achieve this goal, global emissions need to be reduced by 45% by 2030 compared to 2010 levels.\n",
      "\n",
      "**Point 2: Carbon pricing is a necessary step**: Implementing effective carbon pricing mechanisms, such as carbon taxes or cap-and-trade systems, is essential for reducing greenhouse gas emissions and financing climate resilience. Carbon pricing puts a direct cost on pollution, discouraging companies from burning fossil fuels and promoting clean energy alternatives. A well-designed carbon price can raise significant revenue, which can be used to support low-carbon infrastructure development, green tech innovation, and climate change adaptation efforts.\n",
      "------------------------------------------------------------\n",
      "\n",
      " climate_assistant Response:\n",
      " Here are two key points about global climate policy:\n",
      "\n",
      "**Point 1: Staying under 1.5°C is crucial**: The Paris Agreement sets an ambitious goal of limiting global warming to well below 2°C (3.6°F) and pursue efforts to limit it to 1.5°C (2.7°F) above pre-industrial levels. This threshold is critical because it would help avoid the most catastrophic impacts of climate change, such as sea-level rise, droughts, and extreme weather events. To achieve this goal, global emissions need to be reduced by 45% by 2030 compared to 2010 levels.\n",
      "\n",
      "**Point 2: Carbon pricing is a necessary step**: Implementing effective carbon pricing mechanisms, such as carbon taxes or cap-and-trade systems, is essential for reducing greenhouse gas emissions and financing climate resilience. Carbon pricing puts a direct cost on pollution, discouraging companies from burning fossil fuels and promoting clean energy alternatives. A well-designed carbon price can raise significant revenue, which can be used to support low-carbon infrastructure development, green tech innovation, and climate change adaptation efforts.\n",
      "----------------------------------------\n",
      "\u001b[33mclimate_assistant\u001b[0m (to user_proxy):\n",
      "\n",
      "Here are two key points about global climate policy:\n",
      "\n",
      "**Point 1: Staying under 1.5°C is crucial**: The Paris Agreement sets an ambitious goal of limiting global warming to well below 2°C (3.6°F) and pursue efforts to limit it to 1.5°C (2.7°F) above pre-industrial levels. This threshold is critical because it would help avoid the most catastrophic impacts of climate change, such as sea-level rise, droughts, and extreme weather events. To achieve this goal, global emissions need to be reduced by 45% by 2030 compared to 2010 levels.\n",
      "\n",
      "**Point 2: Carbon pricing is a necessary step**: Implementing effective carbon pricing mechanisms, such as carbon taxes or cap-and-trade systems, is essential for reducing greenhouse gas emissions and financing climate resilience. Carbon pricing puts a direct cost on pollution, discouraging companies from burning fossil fuels and promoting clean energy alternatives. A well-designed carbon price can raise significant revenue, which can be used to support low-carbon infrastructure development, green tech innovation, and climate change adaptation efforts.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Message Chain Protocol Message msg_004 ---\n",
      " 2025-06-21T15:15:58.878070\n",
      "From: user_proxy → To: climate_assistant\n",
      "Thought: Received and acknowledged: Here are two key points about global climate policy:\n",
      "Observation:\n",
      "\n",
      "**Point 1: Staying under 1.5°C is crucial**: T...\n",
      "------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to climate_assistant):\n",
      "\n",
      "Thank you for the information!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Message Chain Protocol Message msg_005 ---\n",
      " 2025-06-21T15:15:58.880674\n",
      "From: user_proxy → To: climate_assistant\n",
      "Thought: Processing request: Thank you for the information!\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Message Chain Protocol Message msg_006 ---\n",
      " 2025-06-21T15:16:02.830620\n",
      "From: climate_assistant → To: user_proxy\n",
      "Thought: Generated response: You're welcome! I'm glad I could help provide a concise overview of two key points about global climate policy. Climate action requires international cooperation and collective effort to address this pressing issue, so it's essential to stay informed and engaged. If you have any more questions or would like to know more about climate policy, feel free to ask!\n",
      "------------------------------------------------------------\n",
      "\n",
      " climate_assistant Response:\n",
      " You're welcome! I'm glad I could help provide a concise overview of two key points about global climate policy. Climate action requires international cooperation and collective effort to address this pressing issue, so it's essential to stay informed and engaged. If you have any more questions or would like to know more about climate policy, feel free to ask!\n",
      "----------------------------------------\n",
      "\u001b[33mclimate_assistant\u001b[0m (to user_proxy):\n",
      "\n",
      "You're welcome! I'm glad I could help provide a concise overview of two key points about global climate policy. Climate action requires international cooperation and collective effort to address this pressing issue, so it's essential to stay informed and engaged. If you have any more questions or would like to know more about climate policy, feel free to ask!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Message Chain Protocol Message msg_007 ---\n",
      " 2025-06-21T15:16:02.833388\n",
      "From: user_proxy → To: climate_assistant\n",
      "Thought: Demo exchange completed successfully\n",
      "------------------------------------------------------------\n",
      "\n",
      " Conversation completed successfully!\n",
      "\n",
      "============================================================\n",
      " Full Message Chain Protocol Exchange Log:\n",
      "============================================================\n",
      "[\n",
      "  {\n",
      "    \"message_id\": \"msg_001\",\n",
      "    \"timestamp\": \"2025-06-21T15:15:40.538940\",\n",
      "    \"turn\": 1,\n",
      "    \"sender\": \"user_proxy\",\n",
      "    \"receiver\": \"climate_assistant\",\n",
      "    \"thought\": \"Initiating conversation: What are two key points about global climate policy?\",\n",
      "    \"tool_call\": null,\n",
      "    \"tool_response\": null,\n",
      "    \"observation\": \"\",\n",
      "    \"state_snapshot\": {\n",
      "      \"is_response\": false,\n",
      "      \"conversation_stage\": \"Turn 1\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"message_id\": \"msg_002\",\n",
      "    \"timestamp\": \"2025-06-21T15:15:40.540212\",\n",
      "    \"turn\": 1,\n",
      "    \"sender\": \"user_proxy\",\n",
      "    \"receiver\": \"climate_assistant\",\n",
      "    \"thought\": \"Processing request: What are two key points about global climate policy?\",\n",
      "    \"tool_call\": null,\n",
      "    \"tool_response\": null,\n",
      "    \"observation\": \"\",\n",
      "    \"state_snapshot\": {\n",
      "      \"is_response\": false,\n",
      "      \"conversation_stage\": \"Turn 1\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"message_id\": \"msg_003\",\n",
      "    \"timestamp\": \"2025-06-21T15:15:58.872625\",\n",
      "    \"turn\": 2,\n",
      "    \"sender\": \"climate_assistant\",\n",
      "    \"receiver\": \"user_proxy\",\n",
      "    \"thought\": \"Generated response: Here are two key points about global climate policy:\",\n",
      "    \"tool_call\": null,\n",
      "    \"tool_response\": null,\n",
      "    \"observation\": \"\\n**Point 1: Staying under 1.5\\u00b0C is crucial**: The Paris Agreement sets an ambitious goal of limiting global warming to well below 2\\u00b0C (3.6\\u00b0F) and pursue efforts to limit it to 1.5\\u00b0C (2.7\\u00b0F) above pre-industrial levels. This threshold is critical because it would help avoid the most catastrophic impacts of climate change, such as sea-level rise, droughts, and extreme weather events. To achieve this goal, global emissions need to be reduced by 45% by 2030 compared to 2010 levels.\\n\\n**Point 2: Carbon pricing is a necessary step**: Implementing effective carbon pricing mechanisms, such as carbon taxes or cap-and-trade systems, is essential for reducing greenhouse gas emissions and financing climate resilience. Carbon pricing puts a direct cost on pollution, discouraging companies from burning fossil fuels and promoting clean energy alternatives. A well-designed carbon price can raise significant revenue, which can be used to support low-carbon infrastructure development, green tech innovation, and climate change adaptation efforts.\",\n",
      "    \"state_snapshot\": {\n",
      "      \"is_response\": true,\n",
      "      \"conversation_stage\": \"Turn 2\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"message_id\": \"msg_004\",\n",
      "    \"timestamp\": \"2025-06-21T15:15:58.878070\",\n",
      "    \"turn\": 2,\n",
      "    \"sender\": \"user_proxy\",\n",
      "    \"receiver\": \"climate_assistant\",\n",
      "    \"thought\": \"Received and acknowledged: Here are two key points about global climate policy:\",\n",
      "    \"tool_call\": null,\n",
      "    \"tool_response\": null,\n",
      "    \"observation\": \"\\n**Point 1: Staying under 1.5\\u00b0C is crucial**: T...\",\n",
      "    \"state_snapshot\": {\n",
      "      \"is_response\": true,\n",
      "      \"conversation_stage\": \"Turn 2\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"message_id\": \"msg_005\",\n",
      "    \"timestamp\": \"2025-06-21T15:15:58.880674\",\n",
      "    \"turn\": 3,\n",
      "    \"sender\": \"user_proxy\",\n",
      "    \"receiver\": \"climate_assistant\",\n",
      "    \"thought\": \"Processing request: Thank you for the information!\",\n",
      "    \"tool_call\": null,\n",
      "    \"tool_response\": null,\n",
      "    \"observation\": \"\",\n",
      "    \"state_snapshot\": {\n",
      "      \"is_response\": false,\n",
      "      \"conversation_stage\": \"Turn 3\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"message_id\": \"msg_006\",\n",
      "    \"timestamp\": \"2025-06-21T15:16:02.830620\",\n",
      "    \"turn\": 3,\n",
      "    \"sender\": \"climate_assistant\",\n",
      "    \"receiver\": \"user_proxy\",\n",
      "    \"thought\": \"Generated response: You're welcome! I'm glad I could help provide a concise overview of two key points about global climate policy. Climate action requires international cooperation and collective effort to address this pressing issue, so it's essential to stay informed and engaged. If you have any more questions or would like to know more about climate policy, feel free to ask!\",\n",
      "    \"tool_call\": null,\n",
      "    \"tool_response\": null,\n",
      "    \"observation\": \"\",\n",
      "    \"state_snapshot\": {\n",
      "      \"is_response\": true,\n",
      "      \"conversation_stage\": \"Turn 3\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"message_id\": \"msg_007\",\n",
      "    \"timestamp\": \"2025-06-21T15:16:02.833388\",\n",
      "    \"turn\": 4,\n",
      "    \"sender\": \"user_proxy\",\n",
      "    \"receiver\": \"climate_assistant\",\n",
      "    \"thought\": \"Demo exchange completed successfully\",\n",
      "    \"tool_call\": null,\n",
      "    \"tool_response\": null,\n",
      "    \"observation\": \"\",\n",
      "    \"state_snapshot\": {\n",
      "      \"is_response\": true,\n",
      "      \"conversation_stage\": \"Turn 4\"\n",
      "    }\n",
      "  }\n",
      "]\n",
      "\n",
      " Total Message Chain Protocol messages logged: 7\n",
      " Message Chain Protocol Exchange Summary:\n",
      "  1. user_proxy → climate_assistant: Initiating conversation: What are two key points a...\n",
      "  2. user_proxy → climate_assistant: Processing request: What are two key points about ...\n",
      "  3. climate_assistant → user_proxy: Generated response: Here are two key points about ...\n",
      "  4. user_proxy → climate_assistant: Received and acknowledged: Here are two key points...\n",
      "  5. user_proxy → climate_assistant: Processing request: Thank you for the information!...\n",
      "  6. climate_assistant → user_proxy: Generated response: You're welcome! I'm glad I cou...\n",
      "  7. user_proxy → climate_assistant: Demo exchange completed successfully...\n",
      "\n",
      " Demo completed!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "# Agentic AI Demo: Message Chain Protocol-Style Exchange Between Two Agents\n",
    "# =============================================================\n",
    "\n",
    "# Imports for structured messaging and time tracking\n",
    "import json\n",
    "from datetime import datetime\n",
    "from autogen import AssistantAgent, UserProxyAgent\n",
    "\n",
    "# === GLOBALS ===\n",
    "\n",
    "# Stores the log of all MCP messages (thought, action, observation)\n",
    "mcp_log = []\n",
    "\n",
    "# Tracks number of messages exchanged in total\n",
    "message_counter = 0\n",
    "\n",
    "# === MCP Logging Function ===\n",
    "\n",
    "def log_mcp_message(msg_id, sender, receiver, content, turn, is_response=False):\n",
    "    \"\"\"\n",
    "    Records a structured Message Chain Protocol message with metadata including\n",
    "    thought, observation, sender, receiver, and timestamp.\n",
    "    \"\"\"\n",
    "    now = datetime.utcnow().isoformat()\n",
    "    lines = content.strip().splitlines() if content else [\"\"]\n",
    "\n",
    "    # First line is treated as the agent's 'thought'\n",
    "    thought = lines[0] if lines else \"\"\n",
    "    # Remaining lines are considered the agent's 'observation'\n",
    "    observation = \"\\n\".join(lines[1:]) if len(lines) > 1 else \"\"\n",
    "\n",
    "    # Construct the MCP message dictionary\n",
    "    mcp_entry = {\n",
    "        \"message_id\": f\"msg_{msg_id:03}\",\n",
    "        \"timestamp\": now,\n",
    "        \"turn\": turn,\n",
    "        \"sender\": sender,\n",
    "        \"receiver\": receiver,\n",
    "        \"thought\": thought,\n",
    "        \"tool_call\": None,         # Placeholder; could log tool name here\n",
    "        \"tool_response\": None,     # Placeholder; could log tool output here\n",
    "        \"observation\": observation,\n",
    "        \"state_snapshot\": {\n",
    "            \"is_response\": is_response,\n",
    "            \"conversation_stage\": f\"Turn {turn}\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Append the entry to the global MCP log\n",
    "    mcp_log.append(mcp_entry)\n",
    "\n",
    "    # Print the message in a readable format\n",
    "    print(f\"\\n--- Message Chain Protocol Message {mcp_entry['message_id']} ---\")\n",
    "    print(f\" {now}\")\n",
    "    print(f\"From: {sender} → To: {receiver}\")\n",
    "    print(f\"Thought: {thought}\")\n",
    "    if observation:\n",
    "        print(f\"Observation:\\n{observation}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# === Local LLM Configuration for Ollama ===\n",
    "\n",
    "llm_config = {\n",
    "    \"config_list\": [\n",
    "        {\n",
    "            \"model\": \"llama3\",  # Update to match the Ollama model you are running\n",
    "            \"base_url\": \"http://localhost:11434/v1\",  # Ollama default endpoint\n",
    "            \"api_key\": \"ollama\",                      # Dummy key for compatibility\n",
    "            \"price\": [0.0, 0.0]                        # Avoids cost warnings\n",
    "        }\n",
    "    ],\n",
    "    \"timeout\": 60,\n",
    "}\n",
    "\n",
    "# === MCP-Enabled Assistant Agent ===\n",
    "\n",
    "class MCPAssistantAgent(AssistantAgent):\n",
    "    \"\"\"\n",
    "    Wraps AssistantAgent to log each incoming and outgoing message as a Message Chain Protocol message.\n",
    "    \"\"\"\n",
    "    def generate_reply(self, messages=None, sender=None, config=None):\n",
    "        global message_counter\n",
    "        if messages is None:\n",
    "            messages = []\n",
    "\n",
    "        # Log the incoming message (agent's 'thought')\n",
    "        if messages:\n",
    "            content = messages[-1].get(\"content\", \"\") if isinstance(messages[-1], dict) else str(messages[-1])\n",
    "            message_counter += 1\n",
    "            log_mcp_message(\n",
    "                msg_id=message_counter,\n",
    "                sender=sender.name if sender and hasattr(sender, 'name') else \"unknown\",\n",
    "                receiver=self.name,\n",
    "                content=f\"Processing request: {content}\",\n",
    "                turn=(message_counter + 1) // 2,\n",
    "                is_response=False\n",
    "            )\n",
    "\n",
    "        # Generate actual response using parent class logic\n",
    "        try:\n",
    "            response = super().generate_reply(messages, sender, config)\n",
    "        except TypeError:\n",
    "            # Handles backwards compatibility for older AutoGen versions\n",
    "            try:\n",
    "                response = super().generate_reply(messages, sender)\n",
    "            except:\n",
    "                response = super().generate_reply(messages)\n",
    "\n",
    "        # Extract the response content for logging\n",
    "        message_counter += 1\n",
    "        if isinstance(response, dict):\n",
    "            response_content = response.get(\"content\", \"\")\n",
    "        elif isinstance(response, str):\n",
    "            response_content = response\n",
    "        else:\n",
    "            response_content = str(response) if response else \"No response generated\"\n",
    "\n",
    "        # Log the assistant's response (observation)\n",
    "        log_mcp_message(\n",
    "            msg_id=message_counter,\n",
    "            sender=self.name,\n",
    "            receiver=sender.name if sender and hasattr(sender, 'name') else \"unknown\",\n",
    "            content=f\"Generated response: {response_content}\",\n",
    "            turn=(message_counter + 1) // 2,\n",
    "            is_response=True\n",
    "        )\n",
    "\n",
    "        # Print the assistant's response to console\n",
    "        print(f\"\\n {self.name} Response:\")\n",
    "        print(f\" {response_content}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        return response\n",
    "\n",
    "# === MCP-Enabled User Agent ===\n",
    "\n",
    "class MCPUserProxyAgent(UserProxyAgent):\n",
    "    \"\"\"\n",
    "    Wraps UserProxyAgent to automatically log responses and simulate a single turn.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.response_count = 0\n",
    "\n",
    "    def generate_reply(self, messages=None, sender=None, config=None):\n",
    "        global message_counter\n",
    "        if messages is None:\n",
    "            messages = []\n",
    "\n",
    "        self.response_count += 1\n",
    "\n",
    "        # Stop after 1 assistant response for demonstration purposes\n",
    "        if self.response_count > 1:\n",
    "            message_counter += 1\n",
    "            log_mcp_message(\n",
    "                msg_id=message_counter,\n",
    "                sender=self.name,\n",
    "                receiver=sender.name if sender and hasattr(sender, 'name') else \"unknown\",\n",
    "                content=\"Demo exchange completed successfully\",\n",
    "                turn=(message_counter + 1) // 2,\n",
    "                is_response=True\n",
    "            )\n",
    "            return None\n",
    "\n",
    "        # Log the assistant message being processed\n",
    "        if messages:\n",
    "            last_message = messages[-1]\n",
    "            content = last_message.get(\"content\", \"\") if isinstance(last_message, dict) else str(last_message)\n",
    "            message_counter += 1\n",
    "            log_mcp_message(\n",
    "                msg_id=message_counter,\n",
    "                sender=self.name,\n",
    "                receiver=sender.name if sender and hasattr(sender, 'name') else \"unknown\",\n",
    "                content=f\"Received and acknowledged: {content[:100]}...\",\n",
    "                turn=(message_counter + 1) // 2,\n",
    "                is_response=True\n",
    "            )\n",
    "\n",
    "        # Continue the conversation (will be auto-replied)\n",
    "        try:\n",
    "            return super().generate_reply(messages, sender, config)\n",
    "        except TypeError:\n",
    "            try:\n",
    "                return super().generate_reply(messages, sender)\n",
    "            except:\n",
    "                return super().generate_reply(messages)\n",
    "\n",
    "# === Instantiate the Agents ===\n",
    "\n",
    "user_proxy = MCPUserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    human_input_mode=\"NEVER\",                      # Prevent interactive prompts\n",
    "    max_consecutive_auto_reply=1,                  # Allow one auto-response\n",
    "    code_execution_config=False,                   # No Docker execution needed\n",
    "    default_auto_reply=\"Thank you for the information!\"\n",
    ")\n",
    "\n",
    "assistant = MCPAssistantAgent(\n",
    "    name=\"climate_assistant\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"You are a helpful assistant focused on climate policy. Provide exactly two key points about the topic in a clear, concise manner.\"\n",
    ")\n",
    "\n",
    "# === Run the MCP Exchange ===\n",
    "\n",
    "print(\"Starting Message Chain Protocol Demo Exchange...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Step 1: Log the user's initial request\n",
    "    initial_message = \"What are two key points about global climate policy?\"\n",
    "    message_counter += 1\n",
    "    log_mcp_message(\n",
    "        msg_id=message_counter,\n",
    "        sender=\"user_proxy\",\n",
    "        receiver=\"climate_assistant\",\n",
    "        content=f\"Initiating conversation: {initial_message}\",\n",
    "        turn=1,\n",
    "        is_response=False\n",
    "    )\n",
    "\n",
    "    print(f\"\\n User Query:\")\n",
    "    print(f\" {initial_message}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # Step 2: Start the conversation\n",
    "    result = user_proxy.initiate_chat(\n",
    "        recipient=assistant,\n",
    "        message=initial_message,\n",
    "        max_turns=3,\n",
    "        silent=False\n",
    "    )\n",
    "\n",
    "    print(\"\\n Conversation completed successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    # If anything fails, show helpful diagnostic message\n",
    "    print(f\" Error during chat: {e}\")\n",
    "    print(\"This might be due to:\")\n",
    "    print(\"- Ollama not running (try: ollama serve)\")\n",
    "    print(\"- Model not available (try: ollama pull llama3)\")\n",
    "    print(\"- Network connectivity issues\")\n",
    "    print(\"- AutoGen version compatibility\")\n",
    "\n",
    "# === Show MCP Message Log ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\" Full Message Chain Protocol Exchange Log:\")\n",
    "print(\"=\" * 60)\n",
    "print(json.dumps(mcp_log, indent=2))\n",
    "\n",
    "print(f\"\\n Total Message Chain Protocol messages logged: {len(mcp_log)}\")\n",
    "print(\" Message Chain Protocol Exchange Summary:\")\n",
    "for i, entry in enumerate(mcp_log, 1):\n",
    "    print(f\"  {i}. {entry['sender']} → {entry['receiver']}: {entry['thought'][:50]}...\")\n",
    "\n",
    "print(\"\\n Demo completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96430da7-263e-47c0-b3c6-57603dd75bb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f07e1b-1c01-4e10-ad1e-791cfacc192c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "610767b0-dd01-4320-a4aa-f144bb7f250d",
   "metadata": {},
   "source": [
    "## Demonstrating Model Context Protocol (MCP) within a Message Chain Protocol (MCP) Workflow\n",
    "\n",
    "### Goal\n",
    "This section of the notebook demonstrates how **Model Context Protocol** (structured tool usage) can be embedded inside a **Message Chain Protocol** (explicit messaging flow) to simulate an LLM that:\n",
    "\n",
    "- Understands a user query\n",
    "- Decides to call a specific tool\n",
    "- Executes that tool (e.g., adds two numbers)\n",
    "- Integrates the result into its final answer\n",
    "- Logs each step for traceability and interpretability\n",
    "\n",
    "---\n",
    "\n",
    "### Step-by-Step Breakdown\n",
    "\n",
    "#### Step 1: Environment Setup\n",
    "- Load your OpenAI API key from a local `.env` file.\n",
    "- Initialize the OpenAI client.\n",
    "\n",
    "#### Step 2: Define Tools (Model Context Protocol)\n",
    "- Specify a JSON-based function (`get_sum`) that takes two numbers `a` and `b` and returns their sum.\n",
    "- This definition follows the **Model Context Protocol**, which provides a schema the model can reason over.\n",
    "\n",
    "#### Step 3: Log User Message (Message Chain Protocol)\n",
    "- Record the user’s message as the first step in the **Message Chain**.\n",
    "- Add metadata like role and timestamp.\n",
    "\n",
    "#### Step 4: Let the Model Decide to Use the Tool\n",
    "- Send the user question to the model.\n",
    "- The model uses the **structured tool definition** to determine it needs to invoke `get_sum`.\n",
    "- It responds with a `tool_call` payload (Model Context Protocol).\n",
    "\n",
    "#### Step 5: Log Assistant’s Tool Call\n",
    "- Record the model's tool call decision in the **Message Chain**.\n",
    "- Include the structured arguments used in the tool call.\n",
    "\n",
    "#### Step 6: Simulate the Tool Execution\n",
    "- Use Python to simulate running the tool (`get_sum(5, 7)`).\n",
    "- Capture and format the result in structured form.\n",
    "\n",
    "#### Step 7: Log Tool Response\n",
    "- Log the tool’s response into the **Message Chain Protocol** as a reply from role `tool`.\n",
    "\n",
    "#### Step 8: Final Model Completion\n",
    "- Send the original query + tool call + result to the model.\n",
    "- The model replies to the user with the final answer: `The sum of 5 and 7 is 12`.\n",
    "\n",
    "#### Step 9: Log Final Response\n",
    "- Log this response in the **Message Chain Protocol**.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaway\n",
    "- **Model Context Protocol** helps structure function/tool interactions.\n",
    "- **Message Chain Protocol** ensures each turn (user → assistant → tool → assistant) is preserved, timestamped, and explainable.\n",
    "\n",
    "This design is ideal for debugging, auditing, simulation, and multi-agent orchestration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cfed66d9-b735-4e3b-a070-b886266fd3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ Message Chain Message msg_001 =================\n",
      " Timestamp: 2025-06-21T15:16:03.567767\n",
      "Role: user\n",
      "Content: What’s the sum of 5 and 7?\n",
      "\n",
      "================ Message Chain Message msg_002 =================\n",
      " Timestamp: 2025-06-21T15:16:04.940147\n",
      "Role: assistant\n",
      "Content: [Tool call issued]\n",
      "\n",
      " Model Context Protocol Payload:\n",
      "{\n",
      "  \"tool_call\": {\n",
      "    \"tool_name\": \"get_sum\",\n",
      "    \"arguments\": {\n",
      "      \"a\": 5,\n",
      "      \"b\": 7\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "================ Message Chain Message msg_003 =================\n",
      " Timestamp: 2025-06-21T15:16:04.940643\n",
      "Role: tool\n",
      "Content: {\"result\": 12}\n",
      "\n",
      " Model Context Protocol Payload:\n",
      "{\n",
      "  \"tool_call_id\": \"call_SQmfT4ruuVEBCsfjqDgIr2Al\",\n",
      "  \"tool_result\": 12\n",
      "}\n",
      "\n",
      "================ Message Chain Message msg_004 =================\n",
      " Timestamp: 2025-06-21T15:16:06.226804\n",
      "Role: assistant\n",
      "Content: The sum of 5 and 7 is 12.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# === STEP 1: Setup ===\n",
    "# Load OpenAI API key from .env (Message Chain Protocol sets environment context)\n",
    "load_dotenv(\"keys.env\")\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# === STEP 2: Define tool using Model Context Protocol ===\n",
    "# Model Context Protocol: Structured function definition for tool use\n",
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_sum\",\n",
    "        \"description\": \"Returns the sum of two numbers\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"a\": {\"type\": \"number\", \"description\": \"First number\"},\n",
    "                \"b\": {\"type\": \"number\", \"description\": \"Second number\"},\n",
    "            },\n",
    "            \"required\": [\"a\", \"b\"]\n",
    "        }\n",
    "    }\n",
    "}]\n",
    "\n",
    "# === STEP 3: Initialize message log for Message Chain Protocol ===\n",
    "# Message Chain Protocol: Tracks full message history, metadata\n",
    "message_log = []\n",
    "message_counter = 0\n",
    "\n",
    "def log_chain_message(role, content, model_context=None):\n",
    "    \"\"\" Message Chain Protocol function to log communication steps\"\"\"\n",
    "    global message_counter\n",
    "    timestamp = datetime.utcnow().isoformat()\n",
    "    message_counter += 1\n",
    "    entry = {\n",
    "        \"id\": f\"msg_{message_counter:03}\",\n",
    "        \"timestamp\": timestamp,\n",
    "        \"role\": role,\n",
    "        \"content\": content,\n",
    "        \"model_context_protocol\": model_context  # Structured model data\n",
    "    }\n",
    "    message_log.append(entry)\n",
    "\n",
    "    # Display with clear distinctions\n",
    "    print(f\"\\n================ Message Chain Message {entry['id']} =================\")\n",
    "    print(f\" Timestamp: {timestamp}\")\n",
    "    print(f\"Role: {role}\")\n",
    "    print(f\"Content: {content}\")\n",
    "    if model_context:\n",
    "        print(\"\\n Model Context Protocol Payload:\")\n",
    "        print(json.dumps(model_context, indent=2))\n",
    "\n",
    "# === STEP 4: User initiates a message ===\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a tool-using assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What’s the sum of 5 and 7?\"}\n",
    "]\n",
    "log_chain_message(\"user\", \"What’s the sum of 5 and 7?\")\n",
    "\n",
    "# === STEP 5: Assistant responds with tool call (structured) ===\n",
    "response_1 = client.chat.completions.create(\n",
    "    model=\"gpt-4-0613\",\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\"\n",
    ")\n",
    "mcp_response_1 = response_1.choices[0].message\n",
    "\n",
    "# Model Context Protocol: Tool call issued in structured format\n",
    "tool_call = mcp_response_1.tool_calls[0]\n",
    "tool_args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "log_chain_message(\"assistant\", \"[Tool call issued]\",\n",
    "    model_context={\n",
    "        \"tool_call\": {\n",
    "            \"tool_name\": tool_call.function.name,\n",
    "            \"arguments\": tool_args\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# === STEP 6: Simulate tool execution ===\n",
    "# Model Context Protocol: Tool function executed externally\n",
    "result = tool_args[\"a\"] + tool_args[\"b\"]\n",
    "tool_result = {\"result\": result}\n",
    "\n",
    "log_chain_message(\"tool\", json.dumps(tool_result),\n",
    "    model_context={\n",
    "        \"tool_call_id\": tool_call.id,\n",
    "        \"tool_result\": result\n",
    "    }\n",
    ")\n",
    "\n",
    "# === STEP 7: Append back into messages (for assistant reply) ===\n",
    "messages.append(mcp_response_1)\n",
    "messages.append({\n",
    "    \"role\": \"tool\",\n",
    "    \"tool_call_id\": tool_call.id,\n",
    "    \"content\": json.dumps(tool_result)\n",
    "})\n",
    "\n",
    "# === STEP 8: Final assistant response ===\n",
    "response_2 = client.chat.completions.create(\n",
    "    model=\"gpt-4-0613\",\n",
    "    messages=messages\n",
    ")\n",
    "final_msg = response_2.choices[0].message.content\n",
    "\n",
    "log_chain_message(\"assistant\", final_msg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ee3206-369e-4c6a-b563-f43bb4d3b2b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
